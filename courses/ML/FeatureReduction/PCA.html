
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Principal Component Analysis &#8212; Dr.Hadi Sadoghi Yazdi</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'courses/ML/FeatureReduction/PCA';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Autoencoders" href="Autoencoders1.html" />
    <link rel="prev" title="Introduction of Feature Reduction" href="FR_Intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../../Home_Page.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/Hadi_Sadoghi_Yazdi.png" class="logo__image only-light" alt="Dr.Hadi Sadoghi Yazdi - Home"/>
    <script>document.write(`<img src="../../../_static/Hadi_Sadoghi_Yazdi.png" class="logo__image only-dark" alt="Dr.Hadi Sadoghi Yazdi - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../Home_Page.html">
                    Welcome to my personal Website!
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../../../Courses.html">Courses</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="../../pattern_recognition.html">Pattern Recognition</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../PR/Introduction/PR_intro.html">Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Introduction/DataSet.html">Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Introduction/Model.html">Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Introduction/Cost.html">Cost_Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Introduction/LearningRule.html">Learning_Rule</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="../../PR/Visualization/PR_intro_Visualization.html">Visualization</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../PR/Clustering/PR_intro_Clustering.html">Clustering Concept</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Clustering/Clustering_1.html">Clustering</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Clustering/FCM_1.html">k-means and fuzzy-c-means clustering</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../PR/Regression/Introduction_Regression.html">Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Regression/Regression_1.html">Linear Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Regression/NonLinearRegression.html">Non-linear Regression: The starting point</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Regression/Linearization.html">Linearization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Regression/Kernel_for_Regression.html">Kernel method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Regression/EvaluationModelSelection.html">Evaluation and Model Selection</a></li>

<li class="toctree-l4"><a class="reference internal" href="../../PR/Regression/Solution_for_Regression.html">Solution Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Regression/TheoryRegression.html">Theoretical Aspects of Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Regression/ApplicationsPatternRecognition.html">Applications in Pattern Recognition</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="../../PR/Classification/PR_intro_Classification.html">Classification</a></li>
</ul>
</details></li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="../../machine_learning.html">Machine Learning</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../Regression/NonParamRegression.html">Regression Review</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Regression/KernelRegression.html">Kernel Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Regression/GP_Regression.html">Gaussian Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="FR_Intro.html">Introduction of Feature Reduction</a></li>
<li class="toctree-l3 current active"><a class="current reference internal" href="#">Principal Component Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="Autoencoders1.html">Autoencoders</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../filtering_algorithms.html">Filtering Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../signal_processing.html">Signal Processing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../image_processing.html">Image Processing</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../PRLabProduction.html">Pattern Lab Production</a></li>



<li class="toctree-l1"><a class="reference internal" href="../../../Contact_Me.html">Contact Me</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../About_Me.html">About Me</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/h-sadoghi/dr-sadoghi.git" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/h-sadoghi/dr-sadoghi.git/issues/new?title=Issue%20on%20page%20%2Fcourses/ML/FeatureReduction/PCA.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/courses/ML/FeatureReduction/PCA.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Principal Component Analysis</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-formulation">Mathematical Formulation:</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-vector">Residual Vector:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-value-of-the-residual-norm">Expected Value of the Residual Norm:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simplification">Simplification:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#minimizing-j">Minimizing <span class="math notranslate nohighlight">\( J \)</span>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-derivative-of-j-with-respect-to-b-i">Partial Derivative of <span class="math notranslate nohighlight">\( J \)</span> with respect to <span class="math notranslate nohighlight">\( b_i \)</span>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#substitution-and-simplification-of-j">Substitution and Simplification of <span class="math notranslate nohighlight">\( J \)</span>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lagrangian-formulation">Lagrangian Formulation:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-derivative-with-respect-to-phi-i">Partial Derivative with Respect to <span class="math notranslate nohighlight">\( \phi_i \)</span>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#minimizing-j-with-the-covariance-matrix-substitution">Minimizing <span class="math notranslate nohighlight">\( J \)</span> with the Covariance Matrix Substitution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pca-code">PCA code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eigenvalue-spread-in-pca">Eigenvalue Spread in PCA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-definition">Mathematical Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation">Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-in-pca">Importance in PCA</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pca-on-high-dimensional-data">PCA on High Dimensional Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-background">Mathematical Background</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#denoising-with-pca">Denoising with PCA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#center-the-data-by-subtracting-the-mean">Center the Data by Subtracting the Mean</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-the-covariance-matrix-of-the-centered-data">Compute the Covariance Matrix of the Centered Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#apply-pca-to-the-centered-data">Apply PCA to the Centered Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reconstruct-the-data-using-the-principal-components">Reconstruct the Data Using the Principal Components</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimate-noise">Estimate Noise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#some-notes-in-denoising-with-pca">Some notes in Denoising with PCA</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="principal-component-analysis">
<h1>Principal Component Analysis<a class="headerlink" href="#principal-component-analysis" title="Link to this heading">#</a></h1>
<p>Principal Component Analysis (PCA) in the context of reducing the dimensionality of a dataset <span class="math notranslate nohighlight">\( x \)</span>, where <span class="math notranslate nohighlight">\( x \)</span> is a <span class="math notranslate nohighlight">\( d \)</span>-dimensional vector and <span class="math notranslate nohighlight">\( \phi_i \)</span> are the principal vectors (eigenvectors) in the <span class="math notranslate nohighlight">\( d \)</span>-dimensional space.</p>
<p><strong>Central Idea of PCA:</strong></p>
<p>PCA aims to transform a dataset <span class="math notranslate nohighlight">\( x \)</span> consisting of <span class="math notranslate nohighlight">\( n \)</span> data points, each represented as a <span class="math notranslate nohighlight">\( d \)</span>-dimensional vector, into a new coordinate system where the axes (principal components) are orthogonal (or orthonormal) and ordered by the amount of variance they explain in the data.</p>
<p><strong>Key Concepts:</strong></p>
<ul class="simple">
<li><p><strong><em>Principal Vectors (<span class="math notranslate nohighlight">\( \phi_i \)</span>)</em></strong>: These are <span class="math notranslate nohighlight">\( \phi_i \)</span> vector that will obtain in optimum state that are the eigenvectors of the covariance matrix <span class="math notranslate nohighlight">\( \Sigma \)</span> of <span class="math notranslate nohighlight">\( x \)</span>. They represent the directions along which the data varies the most (principal directions).</p></li>
<li><p><strong><em>Orthogonality and Normalization</em></strong>:</p>
<ul>
<li><p>We assume that The principal vectors <span class="math notranslate nohighlight">\( \phi_i \)</span> are typically chosen to be orthogonal (<span class="math notranslate nohighlight">\( \phi_i^T \phi_j = 0 \)</span> for <span class="math notranslate nohighlight">\( i \neq j \)</span>).</p></li>
<li><p>In some cases, they are also normalized (<span class="math notranslate nohighlight">\( \|\phi_i\| = 1 \)</span>), forming an orthonormal basis.</p></li>
</ul>
</li>
<li><p><strong><em>Linear Combination</em></strong>:</p>
<ul>
<li><p>Each data point <span class="math notranslate nohighlight">\( x \)</span> can be expressed as a linear combination of the principal vectors <span class="math notranslate nohighlight">\( \phi_i \)</span>:
$<span class="math notranslate nohighlight">\(
x = \sum_{i=1}^{d} y_i \phi_i
\)</span><span class="math notranslate nohighlight">\(
where \)</span> y_i = \phi_i^T x <span class="math notranslate nohighlight">\( are the coordinates (or projections) of \)</span> x <span class="math notranslate nohighlight">\( onto the principal components \)</span> \phi_i $.</p></li>
</ul>
</li>
<li><p><strong><em>Dimensionality Reduction</em></strong>:</p>
<ul>
<li><p>To reduce the dimensionality of <span class="math notranslate nohighlight">\( x \)</span>, we can truncate the series after <span class="math notranslate nohighlight">\( m \)</span> principal components, retaining the most significant directions that capture the most variance in the data.</p></li>
<li><p>The reduced representation of <span class="math notranslate nohighlight">\( x \)</span> using the first <span class="math notranslate nohighlight">\( m \)</span> principal components is given by:
$<span class="math notranslate nohighlight">\(
\hat{x} = \sum_{i=1}^{m} y_i \phi_i
\)</span>$</p></li>
</ul>
</li>
</ul>
<section id="mathematical-formulation">
<h2>Mathematical Formulation:<a class="headerlink" href="#mathematical-formulation" title="Link to this heading">#</a></h2>
<p>Given <span class="math notranslate nohighlight">\( x \)</span> is a <span class="math notranslate nohighlight">\( d \)</span>-dimensional vector and <span class="math notranslate nohighlight">\( \phi_i \)</span> are <span class="math notranslate nohighlight">\( d \)</span>-dimensional principal vectors:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( x \)</span>: Original data vector in <span class="math notranslate nohighlight">\( \mathbb{R}^d \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( \phi_i \)</span>: Principal vectors (eigenvectors) in <span class="math notranslate nohighlight">\( \mathbb{R}^d \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( y_i = \phi_i^T x \)</span>: Scalar projections of <span class="math notranslate nohighlight">\( x \)</span> onto <span class="math notranslate nohighlight">\( \phi_i \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( \hat{x} \)</span>: Approximation of <span class="math notranslate nohighlight">\( x \)</span> using <span class="math notranslate nohighlight">\( m \)</span> principal components.</p></li>
</ul>
<section id="residual-vector">
<h3>Residual Vector:<a class="headerlink" href="#residual-vector" title="Link to this heading">#</a></h3>
<p>The residual vector <span class="math notranslate nohighlight">\( \delta x \)</span> is the difference between <span class="math notranslate nohighlight">\( x \)</span> and its projection <span class="math notranslate nohighlight">\( \hat{x} \)</span>:</p>
<div class="math notranslate nohighlight">
\[
\delta x = x - \hat{x} = \sum_{i=m+1}^{d} (y_i - b_i) \phi_i
\]</div>
<p>Here, <span class="math notranslate nohighlight">\( b_i \)</span> are coefficients chosen to minimize the residual <span class="math notranslate nohighlight">\( E\left\{ \left\| \delta x  \right\|^{2} \right\}
 \)</span>, representing the remaining components not captured by the first <span class="math notranslate nohighlight">\( m \)</span> principal components.</p>
</section>
<section id="expected-value-of-the-residual-norm">
<h3>Expected Value of the Residual Norm:<a class="headerlink" href="#expected-value-of-the-residual-norm" title="Link to this heading">#</a></h3>
<p>To find the expected value of <span class="math notranslate nohighlight">\( \| \delta x \| \)</span>, we consider the expectation over the residual components:</p>
<div class="math notranslate nohighlight">
\[
J = E[\| \delta x \|^2] = E\left[\left\| \sum_{i=m+1}^{d} (y_i - b_i) \phi_i \right\|^2\right]
\]</div>
<p>Since <span class="math notranslate nohighlight">\( \phi_i \)</span> are orthonormal, <span class="math notranslate nohighlight">\( \| \phi_i \| = 1 \)</span>.</p>
</section>
<section id="simplification">
<h3>Simplification:<a class="headerlink" href="#simplification" title="Link to this heading">#</a></h3>
<p>The norm squared of <span class="math notranslate nohighlight">\( \delta x \)</span> can be expressed as:</p>
<div class="math notranslate nohighlight">
\[
E\left[\left\| \sum_{i=m+1}^{d} (y_i - b_i) \phi_i \right\|^2\right]
\]</div>
<div class="math notranslate nohighlight">
\[
E\left\{ (\sum_{i=m+1}^{i=d} (y_i - b_i) \phi_i))^{T}(\sum_{i=m+1}^{i=d} (y_i - b_i) \phi_i)) \right\}
\]</div>
<p>because of orthogonality set of <span class="math notranslate nohighlight">\(\phi_i\)</span></p>
<div class="math notranslate nohighlight">
\[
E\| \delta x \|^2=\sum_{i=m+1}^{i=d} E(y_i - b_i)^{2} \phi_i^{T}\phi_i
\]</div>
<p>Since <span class="math notranslate nohighlight">\( \| \phi_i \|^2 = 1 \)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
\| \delta x \|^2 = \sum_{i=m+1}^{d} E(y_i - b_i)^2
\]</div>
</section>
<section id="minimizing-j">
<h3>Minimizing <span class="math notranslate nohighlight">\( J \)</span>:<a class="headerlink" href="#minimizing-j" title="Link to this heading">#</a></h3>
<p>To minimize <span class="math notranslate nohighlight">\( J = E[\| \delta x \|] \)</span>, we choose <span class="math notranslate nohighlight">\( b_i \)</span> such that <span class="math notranslate nohighlight">\( b_i = E(y_i) \)</span>, which minimizes expectation of the residual norm <span class="math notranslate nohighlight">\( \| \delta x \| \)</span>.</p>
</section>
<section id="partial-derivative-of-j-with-respect-to-b-i">
<h3>Partial Derivative of <span class="math notranslate nohighlight">\( J \)</span> with respect to <span class="math notranslate nohighlight">\( b_i \)</span>:<a class="headerlink" href="#partial-derivative-of-j-with-respect-to-b-i" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Express <span class="math notranslate nohighlight">\( J \)</span> in Terms of <span class="math notranslate nohighlight">\( b_i \)</span></strong>:
$<span class="math notranslate nohighlight">\(
J = \sum_{i=m+1}^{d} \mathbb{E}[(y_i - b_i)^2]
\)</span>$</p></li>
<li><p><strong>Differentiate <span class="math notranslate nohighlight">\( J \)</span> with respect to <span class="math notranslate nohighlight">\( b_i \)</span></strong>:
To find the derivative of <span class="math notranslate nohighlight">\( J \)</span> with respect to <span class="math notranslate nohighlight">\( b_i \)</span>, we consider the derivative inside the expectation:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\frac{\partial J}{\partial b_i} = \frac{\partial}{\partial b_i} \left( \mathbb{E}[(y_i - b_i)^2] \right)
\]</div>
<ol class="arabic simple" start="3">
<li><p><strong>Apply Chain Rule</strong>:
Using the chain rule, we get:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\frac{\partial J}{\partial b_i} = -2 \mathbb{E}[y_i - b_i]
\]</div>
<ol class="arabic simple" start="4">
<li><p><strong>Expectation Simplification</strong>:
Since <span class="math notranslate nohighlight">\( y_i \)</span> is deterministic (not random) and <span class="math notranslate nohighlight">\( b_i \)</span> is the variable with respect to which we are differentiating:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\frac{\partial J}{\partial b_i} = -2 \mathbb{E}[y_i - b_i]
\]</div>
<p><span class="math notranslate nohighlight">\( b_i = E(y_i) \)</span></p>
</section>
<section id="substitution-and-simplification-of-j">
<h3>Substitution and Simplification of <span class="math notranslate nohighlight">\( J \)</span>:<a class="headerlink" href="#substitution-and-simplification-of-j" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Substitute <span class="math notranslate nohighlight">\( y_i = \phi_i^T x \)</span> and <span class="math notranslate nohighlight">\( b_i = \phi_i^T \mathbb{E}[x] \)</span></strong> into <span class="math notranslate nohighlight">\( J \)</span>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
J = \sum_{i=m+1}^{d} (y_i - \mathbb{E}[y_i])^2
\]</div>
<div class="math notranslate nohighlight">
\[
J = \sum_{i=m+1}^{d} \left( \phi_i^T x - \phi_i^T \mathbb{E}[x] \right)^2
\]</div>
<ol class="arabic simple" start="2">
<li><p><strong>Expand the square</strong> and use the fact that <span class="math notranslate nohighlight">\( \phi_i^T \)</span> is a scalar (transpose of <span class="math notranslate nohighlight">\( \phi_i \)</span>):</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
J = \sum_{i=m+1}^{d} \left( \phi_i^T (x - \mathbb{E}[x]) \right)^2
\]</div>
<ol class="arabic simple" start="3">
<li><p><strong>Factor out <span class="math notranslate nohighlight">\( (x - \mathbb{E}[x]) \)</span></strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
J = \sum_{i=m+1}^{d} \left( \phi_i^T (x - \mathbb{E}[x]) \right)^2
\]</div>
<div class="math notranslate nohighlight">
\[
J = \sum_{i=m+1}^{d} \phi_i^T (x - \mathbb{E}[x])(x - \mathbb{E}[x])^T \phi_i
\]</div>
<ol class="arabic" start="4">
<li><p><strong>Simplify using matrix multiplication</strong>:</p>
<p>Notice that <span class="math notranslate nohighlight">\( (x - \mathbb{E}[x])(x - \mathbb{E}[x])^T \)</span> is the covariance matrix <span class="math notranslate nohighlight">\( \Sigma_x \)</span> of <span class="math notranslate nohighlight">\( x \)</span>:</p>
</li>
</ol>
<div class="math notranslate nohighlight">
\[
J = \sum_{i=m+1}^{d} \phi_i^T \Sigma_x \phi_i
\]</div>
<p>Here, <span class="math notranslate nohighlight">\( \Sigma_x = \mathbb{E}[(x - \mathbb{E}[x])(x - \mathbb{E}[x])^T] \)</span> is the covariance matrix of <span class="math notranslate nohighlight">\( x \)</span>.</p>
<p>To find the Lagrangian for <span class="math notranslate nohighlight">\( J = \sum_{i=m+1}^{d} \phi_i^T \Sigma_x \phi_i \)</span> with the constraint <span class="math notranslate nohighlight">\( \phi_i^T \phi_i = 1 \)</span>, and then compute the partial derivative with respect to <span class="math notranslate nohighlight">\( \phi_i \)</span>, follow these steps:</p>
</section>
<section id="lagrangian-formulation">
<h3>Lagrangian Formulation:<a class="headerlink" href="#lagrangian-formulation" title="Link to this heading">#</a></h3>
<ol class="arabic">
<li><p><strong>Lagrangian <span class="math notranslate nohighlight">\( \mathcal{L} \)</span></strong>:
$<span class="math notranslate nohighlight">\(
\mathcal{L}(\phi_i, \lambda_i) = \sum_{i=m+1}^{d} \phi_i^T \Sigma_x \phi_i + \sum_{i=m+1}^{d} \lambda_i (1 - \phi_i^T \phi_i)
\)</span>$</p>
<p>Here, <span class="math notranslate nohighlight">\( \lambda_i \)</span> are Lagrange multipliers associated with the constraints <span class="math notranslate nohighlight">\( \phi_i^T \phi_i = 1 \)</span>.</p>
</li>
<li><p><strong>Objective Function with Constraints</strong>:
The objective is to minimize <span class="math notranslate nohighlight">\( \mathcal{L} \)</span> with respect to <span class="math notranslate nohighlight">\( \phi_i \)</span> under the orthogonality and normalization constraints.</p></li>
</ol>
</section>
<section id="partial-derivative-with-respect-to-phi-i">
<h3>Partial Derivative with Respect to <span class="math notranslate nohighlight">\( \phi_i \)</span>:<a class="headerlink" href="#partial-derivative-with-respect-to-phi-i" title="Link to this heading">#</a></h3>
<p>To find <span class="math notranslate nohighlight">\( \frac{\partial \mathcal{L}}{\partial \phi_i} \)</span>:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \phi_i} = 2 \Sigma_x \phi_i - 2 \lambda_i \phi_i
\]</div>
<p>Set the derivative equal to zero to find the critical point:</p>
<div class="math notranslate nohighlight">
\[
2 \Sigma_x \phi_i - 2 \lambda_i \phi_i = 0
\]</div>
<div class="math notranslate nohighlight">
\[
\Sigma_x \phi_i = \lambda_i \phi_i
\]</div>
<p>This equation resembles the eigenvalue problem where <span class="math notranslate nohighlight">\( \lambda_i \)</span> are the Lagrange multipliers, which correspond to the eigenvalues of <span class="math notranslate nohighlight">\( \Sigma_x \)</span>, and <span class="math notranslate nohighlight">\( \phi_i \)</span> are the corresponding eigenvectors.</p>
</section>
<section id="minimizing-j-with-the-covariance-matrix-substitution">
<h3>Minimizing <span class="math notranslate nohighlight">\( J \)</span> with the Covariance Matrix Substitution<a class="headerlink" href="#minimizing-j-with-the-covariance-matrix-substitution" title="Link to this heading">#</a></h3>
<p>To find the minimum value of <span class="math notranslate nohighlight">\( J \)</span>, given the substitution <span class="math notranslate nohighlight">\( \Sigma_x \phi_i = \lambda_i \phi_i \)</span>, follow these steps:</p>
<p><strong>Lagrangian Substitution</strong>:</p>
<p>Recall that <span class="math notranslate nohighlight">\( J \)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
J = \sum_{i=m+1}^{d} \phi_i^T \Sigma_x \phi_i
\]</div>
<p>Using the eigenvalue equation <span class="math notranslate nohighlight">\( \Sigma_x \phi_i = \lambda_i \phi_i \)</span>, we substitute <span class="math notranslate nohighlight">\( \Sigma_x \phi_i \)</span> in <span class="math notranslate nohighlight">\( J \)</span>:</p>
<div class="math notranslate nohighlight">
\[
J = \sum_{i=m+1}^{d} \phi_i^T (\lambda_i \phi_i)
\]</div>
<p>Since <span class="math notranslate nohighlight">\( \phi_i^T \phi_i = 1 \)</span>:</p>
<div class="math notranslate nohighlight">
\[
J = \sum_{i=m+1}^{d} \lambda_i \phi_i^T \phi_i
\]</div>
<div class="math notranslate nohighlight">
\[
J = \sum_{i=m+1}^{d} \lambda_i
\]</div>
<p><strong>Selecting Principal Components</strong>:</p>
<p>The objective is to minimize <span class="math notranslate nohighlight">\( J \)</span>, which sums the eigenvalues <span class="math notranslate nohighlight">\( \lambda_i \)</span> for <span class="math notranslate nohighlight">\( i &gt; m \)</span>. To achieve this:</p>
<ul class="simple">
<li><p><strong>Retain the Largest Eigenvalues</strong>: We retain the principal components corresponding to the largest eigenvalues (those associated with the first <span class="math notranslate nohighlight">\( m \)</span> principal components). These eigenvalues capture the most variance in the data.</p></li>
<li><p><strong>Delete the Smallest Eigenvalues</strong>: The smallest eigenvalues, which are the remaining eigenvalues after the first <span class="math notranslate nohighlight">\( m \)</span> components, are effectively minimized in the residual vector. Hence, these smaller eigenvalues should be excluded to minimize <span class="math notranslate nohighlight">\( J \)</span>.</p></li>
</ul>
</section>
<section id="pca-code">
<h3>PCA code<a class="headerlink" href="#pca-code" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Generate some sample data (for demonstration purposes)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># 100 samples, 2 features for 2-D scatter plot</span>

<span class="c1"># Standardize the data</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">data_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Perform PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_scaled</span><span class="p">)</span>

<span class="c1"># Explained variance by each principal component</span>
<span class="n">explained_variance</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span>
<span class="n">explained_variance_ratio</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>

<span class="c1"># Eigenvectors (principal components)</span>
<span class="n">principal_components</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span>

<span class="c1"># 2-D Scatter plot of the training data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_scaled</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_scaled</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training Data&#39;</span><span class="p">)</span>

<span class="c1"># Plotting the eigenvectors</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">eigenvector</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">principal_components</span><span class="p">,</span> <span class="n">explained_variance</span><span class="p">)):</span>
    <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">eigenvector</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variance</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>  <span class="c1"># Scale eigenvectors by sqrt of eigenvalues</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="o">*</span><span class="n">start</span><span class="p">,</span> <span class="o">*</span><span class="n">end</span><span class="p">,</span> <span class="n">angles</span><span class="o">=</span><span class="s1">&#39;xy&#39;</span><span class="p">,</span> <span class="n">scale_units</span><span class="o">=</span><span class="s1">&#39;xy&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;PC </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1"> (λ=</span><span class="si">{</span><span class="n">variance</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>

<span class="c1"># Annotate explained variance ratio</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;Explained Variance Ratio:</span><span class="se">\n</span><span class="s1">PC1: </span><span class="si">{</span><span class="n">explained_variance_ratio</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">%</span><span class="se">\n</span><span class="s1">PC2: </span><span class="si">{</span><span class="n">explained_variance_ratio</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">,</span> 
         <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;grey&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;grey&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Principal Component 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Principal Component 2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;2-D Scatter Plot with Principal Components&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/7a374385218a94e85c59cd02421119811455832ccfa27e8d31d5a57bc4b9cce3.png" src="../../../_images/7a374385218a94e85c59cd02421119811455832ccfa27e8d31d5a57bc4b9cce3.png" />
</div>
</div>
</section>
<section id="eigenvalue-spread-in-pca">
<h3>Eigenvalue Spread in PCA<a class="headerlink" href="#eigenvalue-spread-in-pca" title="Link to this heading">#</a></h3>
<p>In Principal Component Analysis (PCA), the <strong>eigenvalue spread</strong> is defined as the ratio of the largest eigenvalue (<span class="math notranslate nohighlight">\(\lambda_{\max}\)</span>) to the smallest eigenvalue (<span class="math notranslate nohighlight">\(\lambda_{\min}\)</span>) of the covariance matrix of the dataset. This ratio provides insight into the variability captured by the principal components and the dimensionality of the data.</p>
</section>
<section id="mathematical-definition">
<h3>Mathematical Definition<a class="headerlink" href="#mathematical-definition" title="Link to this heading">#</a></h3>
<p>Given the covariance matrix <span class="math notranslate nohighlight">\(\Sigma_x\)</span> of the dataset <span class="math notranslate nohighlight">\(X\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\Sigma_x = \frac{1}{n-1} X_{centered}^T X_{centered}
\]</div>
<p>where <span class="math notranslate nohighlight">\(X_{centered}\)</span> is the centered data matrix (i.e., the mean of each feature subtracted from the data).</p>
<p>The eigenvalues <span class="math notranslate nohighlight">\(\lambda_1, \lambda_2, \ldots, \lambda_d\)</span> of the covariance matrix are found by solving the eigenvalue equation:</p>
<div class="math notranslate nohighlight">
\[
\Sigma_x \phi_i = \lambda_i \phi_i
\]</div>
<p>where <span class="math notranslate nohighlight">\(\phi_i\)</span> are the corresponding eigenvectors.</p>
<p>The eigenvalue spread is given by:</p>
<div class="math notranslate nohighlight">
\[
\text{Eigenvalue Spread} = \frac{\lambda_{\max}}{\lambda_{\min}}
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda_{\max}\)</span> is the largest eigenvalue.</p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda_{\min}\)</span> is the smallest eigenvalue.</p></li>
</ul>
</section>
<section id="interpretation">
<h3>Interpretation<a class="headerlink" href="#interpretation" title="Link to this heading">#</a></h3>
<p><strong>Large Eigenvalue Spread</strong>: Indicates that the data variance is dominated by a few principal components. This suggests that the dataset is highly anisotropic, meaning the data is stretched more in some directions than others.
<strong>Small Eigenvalue Spread</strong>: Indicates that the data variance is more evenly distributed across the principal components, suggesting a more isotropic dataset.</p>
</section>
<section id="importance-in-pca">
<h3>Importance in PCA<a class="headerlink" href="#importance-in-pca" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Dimensionality Reduction</strong>: A large eigenvalue spread allows for effective dimensionality reduction by selecting a subset of principal components that capture most of the variance in the data.</p></li>
<li><p><strong>Noise and Signal Separation</strong>: A high eigenvalue spread often means that the smaller eigenvalues may represent noise, while the larger eigenvalues capture the significant signal in the data.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Generate synthetic data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>  <span class="c1"># covariance matrix</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Center the data</span>
<span class="n">data_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">data_centered</span> <span class="o">=</span> <span class="n">data</span> <span class="o">-</span> <span class="n">data_mean</span>

<span class="c1"># Compute covariance matrix</span>
<span class="n">cov_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">data_centered</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

<span class="c1"># Eigen decomposition</span>
<span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">cov_matrix</span><span class="p">)</span>

<span class="c1"># Sort eigenvalues and corresponding eigenvectors</span>
<span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">[</span><span class="n">sorted_indices</span><span class="p">]</span>
<span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="p">[:,</span> <span class="n">sorted_indices</span><span class="p">]</span>

<span class="c1"># Plot data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Plot eigenvectors</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)):</span>
    <span class="n">eig_vec</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
    <span class="n">eig_val</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="n">data_mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">data_mean</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">eig_vec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">eig_val</span><span class="p">,</span> <span class="n">eig_vec</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">eig_val</span><span class="p">,</span>
               <span class="n">angles</span><span class="o">=</span><span class="s1">&#39;xy&#39;</span><span class="p">,</span> <span class="n">scale_units</span><span class="o">=</span><span class="s1">&#39;xy&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>

<span class="c1"># Annotate eigenvalues</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)):</span>
    <span class="n">eig_vec</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
    <span class="n">eig_val</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">data_mean</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">eig_vec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">eig_val</span><span class="p">,</span> <span class="n">data_mean</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">eig_vec</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">eig_val</span><span class="p">,</span>
             <span class="sa">f</span><span class="s1">&#39;λ</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">=</span><span class="si">{</span><span class="n">eig_val</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;2D Data with Principal Components&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;X2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/646e2d386d498675cb7a6b03ca835a4065662e42413738f410db812b21e8df92.png" src="../../../_images/646e2d386d498675cb7a6b03ca835a4065662e42413738f410db812b21e8df92.png" />
</div>
</div>
</section>
</section>
<section id="pca-on-high-dimensional-data">
<h2>PCA on High Dimensional Data<a class="headerlink" href="#pca-on-high-dimensional-data" title="Link to this heading">#</a></h2>
<section id="mathematical-background">
<h3>Mathematical Background<a class="headerlink" href="#mathematical-background" title="Link to this heading">#</a></h3>
<p>Given the initial image data <span class="math notranslate nohighlight">\( X \)</span>, the goal is to use PCA for denoising. Let’s go through the derivation.</p>
<ol class="arabic simple">
<li><p><strong>Center the Data</strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
X_{\text{centered}} = X - \mu
\]</div>
<p>where <span class="math notranslate nohighlight">\( \mu \)</span> is the mean of the image data <span class="math notranslate nohighlight">\( X \)</span>.</p>
<ol class="arabic simple" start="2">
<li><p><strong>Compute the Covariance Matrix</strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\Sigma_X = X_{\text{centered}} X_{\text{centered}}^T
\]</div>
<ol class="arabic simple" start="3">
<li><p><strong>Eigenvalue Decomposition</strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\Sigma_X \phi_i = \lambda_i \phi_i
\]</div>
<ol class="arabic simple" start="4">
<li><p><strong>Right Multiply by <span class="math notranslate nohighlight">\( X_{\text{centered}}^T \)</span></strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\Sigma_X \phi_i = \lambda_i \phi_i
\]</div>
<div class="math notranslate nohighlight">
\[
X_{\text{centered}}^T \Sigma_X \phi_i = \lambda_i X_{\text{centered}}^T \phi_i
\]</div>
<p>Substitute <span class="math notranslate nohighlight">\( \hat{\phi_i} = X_{\text{centered}}^T \phi_i \)</span>:</p>
<div class="math notranslate nohighlight">
\[
X_{\text{centered}}^T X_{\text{centered}} X_{\text{centered}}^T \phi_i = \lambda_i X_{\text{centered}}^T \phi_i
\]</div>
<div class="math notranslate nohighlight">
\[
X_{\text{centered}}^T X_{\text{centered}} \hat{\phi_i} = \lambda_i \hat{\phi_i}
\]</div>
<p><span class="math notranslate nohighlight">\( \hat{\phi_i} for i=1:n \)</span> (n is number of samples) calculates which are eigen vector of</p>
<div class="math notranslate nohighlight">
\[
X_{\text{centered}}^T X_{\text{centered}}
\]</div>
<p>on the other hand, multiply from left at following equation X</p>
<div class="math notranslate nohighlight">
\[
X_{\text{centered}}^T X_{\text{centered}} \hat{\phi_i} = \lambda_i \hat{\phi_i}
\]</div>
<p>result:</p>
<div class="math notranslate nohighlight">
\[
X_{\text{centered}}X_{\text{centered}}^T X_{\text{centered}} \hat{\phi_i} = \lambda_i X_{\text{centered}} \hat{\phi_i}
\]</div>
<p>So</p>
<div class="math notranslate nohighlight">
\[
\phi_i=X_{\text{centered}} \hat{\phi_i}
\]</div>
<p>Therefore after obtaining <span class="math notranslate nohighlight">\( \hat{\phi_i} \)</span> using above equation can calculate <span class="math notranslate nohighlight">\(\phi_i\)</span>.</p>
<p><strong>Reconstruct the Image</strong>:
$<span class="math notranslate nohighlight">\(
\hat{X} = \hat{\phi} Y + \mu
\)</span>$</p>
</section>
</section>
<section id="denoising-with-pca">
<h2>Denoising with PCA<a class="headerlink" href="#denoising-with-pca" title="Link to this heading">#</a></h2>
<p>The following steps needs for data denoising using PCA:</p>
<ul class="simple">
<li><p>Center the data by subtracting the mean.</p></li>
<li><p>Compute the covariance matrix of the centered data.</p></li>
<li><p>Apply PCA to the centered data.</p></li>
<li><p>Reconstruct the data using the principal components and stored mean.</p></li>
</ul>
<section id="center-the-data-by-subtracting-the-mean">
<h3>Center the Data by Subtracting the Mean<a class="headerlink" href="#center-the-data-by-subtracting-the-mean" title="Link to this heading">#</a></h3>
<p>Given a dataset <span class="math notranslate nohighlight">\( \mathbf{X} \)</span> with <span class="math notranslate nohighlight">\( n \)</span> samples and <span class="math notranslate nohighlight">\( d \)</span> features, we first compute the mean of the data:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{\mu} = \frac{1}{n} \sum_{i=1}^n \mathbf{x}_i
\]</div>
<p>We then center the data by subtracting the mean vector from each data point:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X}_{\text{centered}} = \mathbf{X} - \mathbf{\mu}
\]</div>
</section>
<section id="compute-the-covariance-matrix-of-the-centered-data">
<h3>Compute the Covariance Matrix of the Centered Data<a class="headerlink" href="#compute-the-covariance-matrix-of-the-centered-data" title="Link to this heading">#</a></h3>
<p>The covariance matrix <span class="math notranslate nohighlight">\( \mathbf{\Sigma} \)</span> of the centered data is calculated as follows:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{\Sigma} = \frac{1}{n} \mathbf{X}_{\text{centered}}^T \mathbf{X}_{\text{centered}}
\]</div>
</section>
<section id="apply-pca-to-the-centered-data">
<h3>Apply PCA to the Centered Data<a class="headerlink" href="#apply-pca-to-the-centered-data" title="Link to this heading">#</a></h3>
<p>Perform PCA by finding the eigenvalues and eigenvectors of the covariance matrix <span class="math notranslate nohighlight">\( \mathbf{\Sigma} \)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{\Sigma} \mathbf{V} = \mathbf{V} \mathbf{\Lambda}
\]</div>
<p>Here, <span class="math notranslate nohighlight">\( \mathbf{V} \)</span> is the matrix of eigenvectors (principal components), and <span class="math notranslate nohighlight">\( \mathbf{\Lambda} \)</span> is the diagonal matrix of eigenvalues.</p>
<p>Transform the centered data to the principal component space:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Z} = \mathbf{X}_{\text{centered}} \mathbf{V}
\]</div>
</section>
<section id="reconstruct-the-data-using-the-principal-components">
<h3>Reconstruct the Data Using the Principal Components<a class="headerlink" href="#reconstruct-the-data-using-the-principal-components" title="Link to this heading">#</a></h3>
<p>To reconstruct the data using the top <span class="math notranslate nohighlight">\( m \)</span> principal components, we truncate the eigenvectors and eigenvalues to include only the top <span class="math notranslate nohighlight">\( m \)</span> components:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{V}_m = \mathbf{V}[:, :m] \]</div>
<div class="math notranslate nohighlight">
\[ \mathbf{\Lambda}_m = \mathbf{\Lambda}[:m, :m]
\]</div>
<p>The reconstructed data in the original space is given by:</p>
<div class="math notranslate nohighlight">
\[
\hat{\mathbf{X}}_{\text{centered}} = \mathbf{Z}_m \mathbf{V}_m^T
\]</div>
<p>Where <span class="math notranslate nohighlight">\( \mathbf{Z}_m = \mathbf{X}_{\text{centered}} \mathbf{V}_m \)</span>.</p>
<p>Finally, add the mean vector back to the reconstructed centered data to get the reconstructed data in the original scale:</p>
<div class="math notranslate nohighlight">
\[
\hat{\mathbf{X}} = \hat{\mathbf{X}}_{\text{centered}} + \mathbf{\mu}
\]</div>
</section>
<section id="estimate-noise">
<h3>Estimate Noise<a class="headerlink" href="#estimate-noise" title="Link to this heading">#</a></h3>
<p>The noise in the data is estimated by subtracting the reconstructed data from the original data:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{N} = \mathbf{X} - \hat{\mathbf{X}}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="c1"># Generate synthetic data with elliptical shape</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_features</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">n_signal</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># Number of signal dimensions</span>

<span class="c1"># Signal component</span>
<span class="n">signal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_signal</span><span class="p">)</span>

<span class="c1"># Create an elliptical distribution by scaling one dimension</span>
<span class="n">scale_factor</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">ellipse_noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">signal</span><span class="p">,</span> <span class="n">scale_factor</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="c1"># Rotate the ellipse to random angle</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span>
<span class="n">rotation_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)],</span>
                            <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span>  <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)]])</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">ellipse_noise</span> <span class="o">@</span> <span class="n">rotation_matrix</span><span class="o">.</span><span class="n">T</span>

<span class="c1"># 1. Center the data by subtracting the mean</span>
<span class="n">mean_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">centered_data</span> <span class="o">=</span> <span class="n">data</span> <span class="o">-</span> <span class="n">mean_data</span>

<span class="c1"># 2. Compute the covariance matrix of the centered data</span>
<span class="n">cov_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">centered_data</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># 3. Apply PCA to the centered data</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">centered_data</span><span class="p">)</span>
<span class="n">components</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">centered_data</span><span class="p">)</span>
<span class="n">explained_variance</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>

<span class="c1"># 4. Reconstruct the data using the top principal components</span>
<span class="n">n_components</span> <span class="o">=</span> <span class="n">n_signal</span>
<span class="n">reconstructed_centered_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">components</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_components</span><span class="p">],</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[:</span><span class="n">n_components</span><span class="p">,</span> <span class="p">:])</span>

<span class="c1"># Add the mean back to get the reconstructed data</span>
<span class="n">reconstructed_data</span> <span class="o">=</span> <span class="n">reconstructed_centered_data</span> <span class="o">+</span> <span class="n">mean_data</span>

<span class="c1"># 5. Estimate noise</span>
<span class="n">noise_estimated</span> <span class="o">=</span> <span class="n">data</span> <span class="o">-</span> <span class="n">reconstructed_data</span>

<span class="c1"># Plot original data, reconstructed data, and noise</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Original and reconstructed data with noise estimation lines</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Original Data&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">reconstructed_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">reconstructed_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Reconstructed Data&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>

<span class="c1"># Draw lines showing the noise estimation</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">reconstructed_data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">reconstructed_data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="s1">&#39;g--&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Original and Reconstructed Data with Noise Estimation&#39;</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Print explained variance</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Explained Variance Ratio:&quot;</span><span class="p">,</span> <span class="n">explained_variance</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/7738084ea0498851c2751608227e2e0f034383c5f008786eb6e40fe162a270b0.png" src="../../../_images/7738084ea0498851c2751608227e2e0f034383c5f008786eb6e40fe162a270b0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Explained Variance Ratio: [0.98929835 0.01070165]
</pre></div>
</div>
</div>
</div>
</section>
<section id="some-notes-in-denoising-with-pca">
<h3>Some notes in Denoising with PCA<a class="headerlink" href="#some-notes-in-denoising-with-pca" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Principal Component Analysis (PCA) is widely used for denoising data due to its ability to capture the underlying structure of the data by analyzing the covariance matrix. In PCA-based denoising, the idea is to exploit the principal components (directions of maximum variance) to approximate the signal while filtering out the noise.</p></li>
<li><p>Local vs. Global Denoising
Local Denoising: Methods like local averaging or filtering consider only neighboring points, which can be sensitive to the local structure of the data but may not capture global patterns.</p></li>
<li><p>Global Denoising with PCA: PCA-based denoising uses global patterns by incorporating the entire dataset to compute the covariance matrix. It projects data points onto global principal components, thereby leveraging the full data structure for noise reduction.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">skimage</span> <span class="kn">import</span> <span class="n">color</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">img_as_float</span>

<span class="c1"># Load and preprocess image</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">img_as_float</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">astronaut</span><span class="p">())</span>  <span class="c1"># Load an example image</span>
<span class="n">gray_image</span> <span class="o">=</span> <span class="n">color</span><span class="o">.</span><span class="n">rgb2gray</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>  <span class="c1"># Convert to grayscale</span>
<span class="n">noisy_image</span> <span class="o">=</span> <span class="n">gray_image</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">gray_image</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Add noise</span>

<span class="c1"># Flatten the images</span>
<span class="n">image_flat</span> <span class="o">=</span> <span class="n">gray_image</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">noisy_image_flat</span> <span class="o">=</span> <span class="n">noisy_image</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="c1"># Create the data matrix (example with multiple noisy images)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">noisy_image_flat</span><span class="p">])</span>  <span class="c1"># In a real scenario, stack multiple images</span>

<span class="c1"># Standardize the data</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Perform PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>  <span class="c1"># Keep enough components to explain 95% variance</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>

<span class="c1"># Reconstruct the image</span>
<span class="n">components</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span>
<span class="n">mean_image</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">mean_</span>

<span class="c1"># Reconstruct the data</span>
<span class="n">X_reconstructed</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">))</span>
<span class="n">reconstructed_image</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_reconstructed</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">gray_image</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Plot original, noisy, and denoised images</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Noisy Image&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">noisy_image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Denoised Image&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">reconstructed_image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Difference&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">noisy_image</span> <span class="o">-</span> <span class="n">reconstructed_image</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;hot&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>E:\MainHomePage\.M_HomePage\Lib\site-packages\sklearn\decomposition\_pca.py:590: RuntimeWarning: invalid value encountered in divide
  explained_variance_ = (S**2) / (n_samples - 1)
</pre></div>
</div>
<img alt="../../../_images/ce2afc45a78d13db1226268871b721400d2709c924d5d6f16205747857d2b963.png" src="../../../_images/ce2afc45a78d13db1226268871b721400d2709c924d5d6f16205747857d2b963.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">skimage</span> <span class="kn">import</span> <span class="n">color</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">img_as_float</span>

<span class="c1"># Load and preprocess image</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">img_as_float</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">astronaut</span><span class="p">())</span>  <span class="c1"># Load an example image</span>
<span class="n">gray_image</span> <span class="o">=</span> <span class="n">color</span><span class="o">.</span><span class="n">rgb2gray</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>  <span class="c1"># Convert to grayscale</span>
<span class="n">noisy_image</span> <span class="o">=</span> <span class="n">gray_image</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">gray_image</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Add noise</span>

<span class="c1"># Flatten the image for PCA</span>
<span class="n">image_flat</span> <span class="o">=</span> <span class="n">gray_image</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">noisy_image_flat</span> <span class="o">=</span> <span class="n">noisy_image</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="c1"># Create multiple noisy samples for PCA</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">35</span> <span class="c1"># Number of samples for PCA</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">noisy_image_flat</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)</span>  <span class="c1"># Simulate multiple noisy samples</span>
<span class="n">X</span> <span class="o">+=</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Add slight variation to simulate multiple noisy images</span>

<span class="c1"># Standardize the data</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Function to reconstruct image with a given number of components</span>
<span class="k">def</span> <span class="nf">reconstruct_image</span><span class="p">(</span><span class="n">n_components</span><span class="p">):</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">)</span>
    <span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
    <span class="n">X_reconstructed</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>
    
    <span class="c1"># Inverse scaling</span>
    <span class="n">X_reconstructed</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_reconstructed</span><span class="p">)</span>
    
    <span class="c1"># Reshape the data back to the image shape</span>
    <span class="n">reconstructed_image</span> <span class="o">=</span> <span class="n">X_reconstructed</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">gray_image</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">reconstructed_image</span>

<span class="c1"># Values of n_components to test</span>
<span class="n">n_components_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">]</span>

<span class="c1"># Plot original, noisy, and denoised images</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">n_components_list</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Noisy Image&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">noisy_image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n_components</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">n_components_list</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">n_components</span> <span class="o">&gt;</span> <span class="nb">min</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="c1"># Skip invalid n_components</span>
        <span class="k">continue</span>

    <span class="n">denoised_image</span> <span class="o">=</span> <span class="n">reconstruct_image</span><span class="p">(</span><span class="n">n_components</span><span class="p">)</span>
    <span class="n">difference</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">noisy_image</span> <span class="o">-</span> <span class="n">denoised_image</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">n_components_list</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Denoised (n=</span><span class="si">{</span><span class="n">n_components</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">denoised_image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">n_components_list</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">n_components_list</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Difference (n=</span><span class="si">{</span><span class="n">n_components</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">difference</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;hot&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/2fae759ad2bb34c520c884e44bfe039d267d8393bf94b2eb03724313f4cd85e1.png" src="../../../_images/2fae759ad2bb34c520c884e44bfe039d267d8393bf94b2eb03724313f4cd85e1.png" />
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./courses\ML\FeatureReduction"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="FR_Intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Introduction of Feature Reduction</p>
      </div>
    </a>
    <a class="right-next"
       href="Autoencoders1.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Autoencoders</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-formulation">Mathematical Formulation:</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-vector">Residual Vector:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-value-of-the-residual-norm">Expected Value of the Residual Norm:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simplification">Simplification:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#minimizing-j">Minimizing <span class="math notranslate nohighlight">\( J \)</span>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-derivative-of-j-with-respect-to-b-i">Partial Derivative of <span class="math notranslate nohighlight">\( J \)</span> with respect to <span class="math notranslate nohighlight">\( b_i \)</span>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#substitution-and-simplification-of-j">Substitution and Simplification of <span class="math notranslate nohighlight">\( J \)</span>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lagrangian-formulation">Lagrangian Formulation:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-derivative-with-respect-to-phi-i">Partial Derivative with Respect to <span class="math notranslate nohighlight">\( \phi_i \)</span>:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#minimizing-j-with-the-covariance-matrix-substitution">Minimizing <span class="math notranslate nohighlight">\( J \)</span> with the Covariance Matrix Substitution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pca-code">PCA code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eigenvalue-spread-in-pca">Eigenvalue Spread in PCA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-definition">Mathematical Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation">Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-in-pca">Importance in PCA</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pca-on-high-dimensional-data">PCA on High Dimensional Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-background">Mathematical Background</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#denoising-with-pca">Denoising with PCA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#center-the-data-by-subtracting-the-mean">Center the Data by Subtracting the Mean</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compute-the-covariance-matrix-of-the-centered-data">Compute the Covariance Matrix of the Centered Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#apply-pca-to-the-centered-data">Apply PCA to the Centered Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reconstruct-the-data-using-the-principal-components">Reconstruct the Data Using the Principal Components</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimate-noise">Estimate Noise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#some-notes-in-denoising-with-pca">Some notes in Denoising with PCA</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dr.Hadi Sadoghi Yazdi
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024 Pattern Recognition Lab.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>