
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Regression Review &#8212; Dr.Hadi Sadoghi Yazdi</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'courses/ML/Regression/NonParamRegression';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Kernel Regression" href="KernelRegression.html" />
    <link rel="prev" title="Machine Learning" href="../../machine_learning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../../Home_Page.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/Hadi_Sadoghi_Yazdi.png" class="logo__image only-light" alt="Dr.Hadi Sadoghi Yazdi - Home"/>
    <script>document.write(`<img src="../../../_static/Hadi_Sadoghi_Yazdi.png" class="logo__image only-dark" alt="Dr.Hadi Sadoghi Yazdi - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../Home_Page.html">
                    Welcome to my personal Website!
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../../../Courses.html">Courses</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="../../pattern_recognition.html">Pattern Recognition</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../PR/Introduction/PR_intro.html">Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Introduction/DataSet.html">Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Introduction/Model.html">Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Introduction/Cost.html">Cost_Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Introduction/LearningRule.html">Learning_Rule</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="../../PR/Visualization/PR_intro_Visualization.html">Visualization</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../PR/Clustering/PR_intro_Clustering.html">Clustering Concept</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Clustering/Clustering_1.html">Clustering</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Clustering/FCM_1.html">k-means and fuzzy-c-means clustering</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../PR/Regression/Introduction_Regression.html">Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Regression/Regression_1.html">Linear Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Regression/NonLinearRegression.html">Non-linear Regression: The starting point</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Regression/Linearization.html">Linearization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Regression/Kernel_for_Regression.html">Kernel method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Regression/EvaluationModelSelection.html">Evaluation and Model Selection</a></li>

<li class="toctree-l4"><a class="reference internal" href="../../PR/Regression/Solution_for_Regression.html">Solution Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Regression/TheoryRegression.html">Theoretical Aspects of Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../PR/Regression/ApplicationsPatternRecognition.html">Applications in Pattern Recognition</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="../../PR/Classification/PR_intro_Classification.html">Classification</a></li>
</ul>
</details></li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="../../machine_learning.html">Machine Learning</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l3 current active"><a class="current reference internal" href="#">Regression Review</a></li>
<li class="toctree-l3"><a class="reference internal" href="KernelRegression.html">Kernel Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="GP_Regression.html">Gaussian Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="../FeatureReduction/FR_Intro.html">Principal Component Analysis</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../filtering_algorithms.html">Filtering Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../signal_processing.html">Signal Processing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../image_processing.html">Image Processing</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../PRLabProduction.html">Pattern Lab Production</a></li>

<li class="toctree-l1"><a class="reference internal" href="../../../Contact_Me.html">Contact Me</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../About_Me.html">About Me</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/h-sadoghi/dr-sadoghi.git" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/h-sadoghi/dr-sadoghi.git/issues/new?title=Issue%20on%20page%20%2Fcourses/ML/Regression/NonParamRegression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/courses/ML/Regression/NonParamRegression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Regression Review</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-specification">Model Specification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#review-of-concepts">Review of concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-model">Regression Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-and-variance-of-the-model">Mean and Variance of the Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-of-the-linear-regression-model">Likelihood of the Linear Regression Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#log-likelihood">Log-Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-for-w">Solving for <span class="math notranslate nohighlight">\( w \)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-for-sigma">Solving for <span class="math notranslate nohighlight">\( \sigma \)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-and-confidence-intervals">Prediction and Confidence Intervals</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-regression">Kernel Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-regression-vs-gaussian-processes">Kernel Regression vs. Gaussian Processes</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Kernel Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#key-points">Key Points:</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-processes">Gaussian Processes</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Key Points:</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-kernel-regression">Introduction to Kernel Regression</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-formulation">Mathematical Formulation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-functions">Kernel Functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bandwidth-selection">Bandwidth Selection</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-and-disadvantages">Advantages and Disadvantages</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-applications">Practical Applications</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#code-example">Code Example</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-process">Gaussian Process</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-for-gp">Example for GP</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-from-probability-density-function-perspective">Regression from Probability Density Function perspective</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#consider-the-example-p-y-theta-with-a-2-variate-y">Consider the example <span class="math notranslate nohighlight">\( p(y|\theta) \)</span> with a 2-variate y</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-conditional-pdf">Visualizing the conditional PDF</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#toward-an-understanding-of-the-process">Toward an understanding of the process</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#y-axis-start-stop">Y-axis start-stop</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#another-representation">Another Representation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#trajectories-as-a-process">Trajectories as a Process</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#x-y-plane-including-the-start-and-stop-of-the-function">X-Y plane (including the start and stop of the function)</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#x-y-plane-including-num-points-per-trajectory-of-the-function">X-Y plane (including num_points_per_trajectory of the function)</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#y-axis-4-points">Y-axis 4-points</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-x-y-plane-k-points">Example X-Y plane k-points</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rbf-kernel-squared-exponential-kernel">RBF Kernel (Squared Exponential Kernel)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-samples">Generating Samples</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-smooth-waves">Why Smooth Waves?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-gaussian-process-from-a-functional-perspective">Define the Gaussian Process from a functional perspective</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-kernel-function">Define the Kernel Function</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#formulate-the-joint-distribution">Formulate the Joint Distribution</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-happens-during-sampling">What Happens During Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-normal-distribution">Multivariate Normal Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-correlated-samples">Generating Correlated Samples</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-process">Sampling Process</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-process-using-quantile-based-distance">Generate process using quantile based distance</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#traditional-rbf-kernel"><strong>Traditional RBF Kernel:</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#quantile-based-distance"><strong>Quantile-Based Distance:</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#impact-of-quantile-based-distance"><strong>Impact of Quantile-Based Distance:</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-generation-with-traditional-rbf-kernel"><strong>Sample Generation with Traditional RBF Kernel:</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-of-epsilon-insensitive-loss-for-process-generation">Explanation of <span class="math notranslate nohighlight">\(\epsilon\)</span>-Insensitive Loss for Process Generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#impact-on-process-generation">Impact on Process Generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mini-project-exploring-smoothness-and-loss-functions-in-gaussian-process-generation">Mini Project: Exploring Smoothness and Loss Functions in Gaussian Process Generation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#objectives">Objectives:</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="regression-review">
<h1>Regression Review<a class="headerlink" href="#regression-review" title="Link to this heading">#</a></h1>
<p>Linear regression is a foundational tool in statistics and supervised machine learning. It’s versatile and powerful, capable of modeling not only linear relationships but also non-linear ones when augmented with techniques such as kernels or basis function expansion. Additionally, by modifying the output distribution from Gaussian to Bernoulli or multinomial, linear regression can be adapted for classification tasks. Therefore, a thorough understanding of this model is essential.</p>
<section id="model-specification">
<h2>Model Specification<a class="headerlink" href="#model-specification" title="Link to this heading">#</a></h2>
<p>Linear regression can be expressed as:
for model to form of <span class="math notranslate nohighlight">\( y=w^{T}x+b+\epsilon \)</span> where <span class="math notranslate nohighlight">\( \epsilon \sim N(0,\sigma^2) \)</span>
$<span class="math notranslate nohighlight">\( p(y|x,\theta) = N(y|w^T x,\sigma^2)  \)</span>$</p>
<p>Here, <span class="math notranslate nohighlight">\( y \)</span> is the target variable, <span class="math notranslate nohighlight">\( x \)</span> represents the input features, <span class="math notranslate nohighlight">\( \theta \)</span> denotes the model parameters, <span class="math notranslate nohighlight">\( w \)</span> is the weight vector, and <span class="math notranslate nohighlight">\( \sigma^2 \)</span> is the variance of the Gaussian noise.</p>
<p>To handle non-linear relationships, we can replace the input <span class="math notranslate nohighlight">\( x \)</span> with a non-linear transformation <span class="math notranslate nohighlight">\( \phi(x) \)</span>. This yields the model:</p>
<div class="math notranslate nohighlight">
\[ p(y|x,\theta) = N(y|w^T \phi(x),\sigma^2)  \]</div>
<p>For now, we modeled the distribution <span class="math notranslate nohighlight">\( p(y|x,\theta) = N(y|w^T \phi(x),\sigma^2) \)</span>. When <span class="math notranslate nohighlight">\(\sigma \to 0\)</span>, this distribution approaches a Dirac Delta function, which minimizes the least squares error. This is equivalent to maximum likelihood estimation under the assumption of Gaussian noise.
An illustrative example of basis function expansion is the use of polynomial basis functions, where the transformed input <span class="math notranslate nohighlight">\( \phi(x) \)</span> is:</p>
<div class="math notranslate nohighlight">
\[ \phi(x) = [1, x, x^2, \ldots, x^d]  \]</div>
<p>following Figure demonstrates how varying the degree <span class="math notranslate nohighlight">\( d \)</span> allows the model to capture increasingly complex relationships.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Function to generate data</span>
<span class="k">def</span> <span class="nf">generate_data</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.15</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="c1"># Function to plot polynomial fits</span>
<span class="k">def</span> <span class="nf">plot_polynomial_fits</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">degrees</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data&#39;</span><span class="p">)</span>
    
    <span class="n">X_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">degree</span> <span class="ow">in</span> <span class="n">degrees</span><span class="p">:</span>
        <span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="p">)</span>
        <span class="n">X_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">y_plot</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">poly</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_plot</span><span class="p">))</span>
        
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_plot</span><span class="p">,</span> <span class="n">y_plot</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Degree </span><span class="si">{</span><span class="n">degree</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Polynomial Basis Function Expansion&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Generate data</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">()</span>

<span class="c1"># Plot polynomial fits for different degrees</span>
<span class="n">degrees</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">13</span><span class="p">]</span>
<span class="n">plot_polynomial_fits</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">degrees</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/39d66aca5ad1dcdef18802d0a113f568943484089e3bc19893c55b8f32f7f54a.png" src="../../../_images/39d66aca5ad1dcdef18802d0a113f568943484089e3bc19893c55b8f32f7f54a.png" />
</div>
</div>
</section>
<section id="review-of-concepts">
<h2>Review of concepts<a class="headerlink" href="#review-of-concepts" title="Link to this heading">#</a></h2>
<p>We follow the model <span class="math notranslate nohighlight">\( p(y|x,\theta) = N(y|w^T x,\sigma^2) \)</span>.</p>
<section id="regression-model">
<h3>Regression Model<a class="headerlink" href="#regression-model" title="Link to this heading">#</a></h3>
<p>The linear regression model can be described as follows:</p>
<div class="math notranslate nohighlight">
\[ y = w_0 x_0 + w_1 x_1 + \cdots + w_D x_D + \epsilon = w \cdot x + \epsilon \]</div>
<p>Here:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y \)</span> is the dependent variable.</p></li>
<li><p><span class="math notranslate nohighlight">\( x = [x_0, x_1, \ldots, x_D] \)</span> is the vector of independent variables (features).</p></li>
<li><p><span class="math notranslate nohighlight">\( w = [w_0, w_1, \ldots, w_D] \)</span> is the vector of weights.</p></li>
<li><p><span class="math notranslate nohighlight">\( \epsilon \)</span> is the noise term, which is assumed to be normally distributed with mean 0 and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p></li>
</ul>
</section>
<section id="mean-and-variance-of-the-model">
<h3>Mean and Variance of the Model<a class="headerlink" href="#mean-and-variance-of-the-model" title="Link to this heading">#</a></h3>
<p>Given <span class="math notranslate nohighlight">\( x \)</span> and <span class="math notranslate nohighlight">\( w \)</span>, the mean of <span class="math notranslate nohighlight">\( y \)</span> is:</p>
<div class="math notranslate nohighlight">
\[ E[y | x, w] = w^T x \]</div>
<p>Given that <span class="math notranslate nohighlight">\( y \)</span> is normally distributed with this mean and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>, the model can be written as:</p>
<div class="math notranslate nohighlight">
\[ p(y | x, w) = N(y | w^T x, \sigma^2) \]</div>
</section>
<section id="likelihood-of-the-linear-regression-model">
<h3>Likelihood of the Linear Regression Model<a class="headerlink" href="#likelihood-of-the-linear-regression-model" title="Link to this heading">#</a></h3>
<p>Suppose we have observed data <span class="math notranslate nohighlight">\(\{(x_i, y_i)\}_{i=1}^N\)</span>. The likelihood of observing this data given the model parameters <span class="math notranslate nohighlight">\( w \)</span> and <span class="math notranslate nohighlight">\( \sigma \)</span> is:</p>
<div class="math notranslate nohighlight">
\[ p(y_1, \ldots, y_N | x_1, \ldots, x_N, w, \sigma) = \prod_{i=1}^N p(y_i | x_i, w, \sigma) \]</div>
<p>Given the model <span class="math notranslate nohighlight">\( y_i \sim w^T x_i + N(0, \sigma^2) \)</span>, the likelihood is:</p>
<div class="math notranslate nohighlight">
\[ p(y_1, \ldots, y_N | x_1, \ldots, x_N, w, \sigma) = \prod_{i=1}^N \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(y_i - w^T x_i)^2}{2\sigma^2} \right) \]</div>
<p>This simplifies to:</p>
<div class="math notranslate nohighlight">
\[ p(y_1, \ldots, y_N | x_1, \ldots, x_N, w, \sigma) = \left( \frac{1}{2\pi\sigma^2} \right)^{N/2} \exp \left( -\frac{1}{2\sigma^2} \sum_{i=1}^N (y_i - w^T x_i)^2 \right) \]</div>
</section>
<section id="log-likelihood">
<h3>Log-Likelihood<a class="headerlink" href="#log-likelihood" title="Link to this heading">#</a></h3>
<p>To simplify the optimization, we often work with the log-likelihood:</p>
<div class="math notranslate nohighlight">
\[ \log p(y_1, \ldots, y_N | x_1, \ldots, x_N, w, \sigma) = -\frac{N}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (y_i - w^T x_i)^2 \]</div>
<p>In vector form, this can be written as:</p>
<div class="math notranslate nohighlight">
\[ \log p(y | X, w, σ) = -\frac{N}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} (Xw - y)^T (Xw - y) \]</div>
</section>
<section id="maximum-likelihood-estimation-mle">
<h3>Maximum Likelihood Estimation (MLE)<a class="headerlink" href="#maximum-likelihood-estimation-mle" title="Link to this heading">#</a></h3>
<p>To find the parameters <span class="math notranslate nohighlight">\( w \)</span> and <span class="math notranslate nohighlight">\( \sigma \)</span> that maximize the likelihood, we can alternatively minimize the negative log-likelihood:</p>
<div class="math notranslate nohighlight">
\[ \text{NLL}(y | X, w, σ) = \frac{1}{2\sigma^2} (Xw - y)^T (Xw - y) + \frac{N}{2} \log(2\pi\sigma^2) \]</div>
<p>This can be compared with the least squares objective function:</p>
<div class="math notranslate nohighlight">
\[ L(w) = \frac{1}{2N} (Xw - y)^T (Xw - y) \]</div>
<p>The solutions to both optimization problems are equivalent up to a constant factor.</p>
</section>
<section id="solving-for-w">
<h3>Solving for <span class="math notranslate nohighlight">\( w \)</span><a class="headerlink" href="#solving-for-w" title="Link to this heading">#</a></h3>
<p>To find the maximum likelihood estimator <span class="math notranslate nohighlight">\( w_{\text{ML}} \)</span>, we solve:</p>
<div class="math notranslate nohighlight">
\[ w_{\text{ML}} = (X^T X)^{-1} X^T y \]</div>
</section>
<section id="solving-for-sigma">
<h3>Solving for <span class="math notranslate nohighlight">\( \sigma \)</span><a class="headerlink" href="#solving-for-sigma" title="Link to this heading">#</a></h3>
<p>The MLE for <span class="math notranslate nohighlight">\(\sigma^2\)</span> is:</p>
<div class="math notranslate nohighlight">
\[ \sigma^2_{\text{ML}} = \frac{1}{N} (Xw_{\text{ML}} - y)^T (Xw_{\text{ML}} - y) \]</div>
</section>
<section id="prediction-and-confidence-intervals">
<h3>Prediction and Confidence Intervals<a class="headerlink" href="#prediction-and-confidence-intervals" title="Link to this heading">#</a></h3>
<p>Given new input <span class="math notranslate nohighlight">\( x_{\text{new}} \)</span>, the prediction is:</p>
<div class="math notranslate nohighlight">
\[ \hat{y}_{\text{new}} = w_{\text{ML}} \cdot x_{\text{new}} \]</div>
<p>The predicted distribution for <span class="math notranslate nohighlight">\( y_{\text{new}} \)</span> is:</p>
<div class="math notranslate nohighlight">
\[ y_{\text{new}} \sim \hat{y}_{\text{new}} + N(0, \sigma^2_{\text{ML}}) \]</div>
<p>This means the prediction has a mean <span class="math notranslate nohighlight">\( \hat{y}_{\text{new}} \)</span> and variance <span class="math notranslate nohighlight">\( \sigma^2_{\text{ML}} \)</span>.</p>
</section>
</section>
<section id="kernel-regression">
<h2>Kernel Regression<a class="headerlink" href="#kernel-regression" title="Link to this heading">#</a></h2>
<p>Kernel regression is a nonparametric technique in statistics that aims to estimate the conditional expectation of a random variable. It smooths the observed data to make predictions about new, unobserved data points.
Kernel regression and Gaussian Processes (GPs) are closely related, but they differ in how they model the data. While kernel regression provides a point estimate <span class="math notranslate nohighlight">\( \hat{y}(x) \)</span>, Gaussian Processes provide a full probabilistic model, offering both a mean function <span class="math notranslate nohighlight">\( m(x) \)</span> and a variance function <span class="math notranslate nohighlight">\( \text{var}(x) \)</span>. Here’s a more detailed comparison and description of kernel regression and Gaussian Processes:</p>
<section id="kernel-regression-vs-gaussian-processes">
<h3>Kernel Regression vs. Gaussian Processes<a class="headerlink" href="#kernel-regression-vs-gaussian-processes" title="Link to this heading">#</a></h3>
<section id="id1">
<h4>Kernel Regression<a class="headerlink" href="#id1" title="Link to this heading">#</a></h4>
<p>Kernel regression is a nonparametric method that estimates the conditional mean of the dependent variable given the independent variable. It uses a kernel function to weigh the nearby data points more heavily than those further away.</p>
<section id="key-points">
<h5>Key Points:<a class="headerlink" href="#key-points" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>Estimation</strong>: Provides a point estimate <span class="math notranslate nohighlight">\( \hat{y}(x) \)</span>.</p></li>
<li><p><strong>Model</strong>: Only models the mean function <span class="math notranslate nohighlight">\( m(x) \)</span>.</p></li>
<li><p><strong>Computation</strong>: Typically less computationally intensive than GPs.</p></li>
<li><p><strong>Bandwidth</strong>: Choice of bandwidth <span class="math notranslate nohighlight">\( h \)</span> is crucial for performance.</p></li>
</ul>
</section>
</section>
<section id="gaussian-processes">
<h4>Gaussian Processes<a class="headerlink" href="#gaussian-processes" title="Link to this heading">#</a></h4>
<p>Gaussian Processes extend the idea of kernel regression by providing a full probabilistic model of the data. A GP defines a distribution over functions and can provide both the mean and the uncertainty of the predictions.</p>
<section id="id2">
<h5>Key Points:<a class="headerlink" href="#id2" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>Estimation</strong>: Provides both a mean function <span class="math notranslate nohighlight">\( m(x) \)</span> and a covariance function <span class="math notranslate nohighlight">\( \text{var}(x) \)</span>.</p></li>
<li><p><strong>Model</strong>: Models the entire distribution of possible functions that fit the data.</p></li>
<li><p><strong>Computation</strong>: Computationally more intensive, especially for large datasets, due to the inversion of the covariance matrix.</p></li>
<li><p><strong>Hyperparameters</strong>: Involves tuning hyperparameters for the kernel, which affects both the mean and covariance predictions.</p></li>
</ul>
</section>
</section>
<section id="introduction-to-kernel-regression">
<h4>Introduction to Kernel Regression<a class="headerlink" href="#introduction-to-kernel-regression" title="Link to this heading">#</a></h4>
<p>Kernel regression estimates the relationship between a dependent variable <span class="math notranslate nohighlight">\( y \)</span> and an independent variable <span class="math notranslate nohighlight">\( x \)</span> by weighing nearby observed data points more heavily than those further away. This weighting is done using a kernel function.</p>
</section>
<section id="mathematical-formulation">
<h4>Mathematical Formulation<a class="headerlink" href="#mathematical-formulation" title="Link to this heading">#</a></h4>
<p>Given a set of data points <span class="math notranslate nohighlight">\( (x_i, y_i) \)</span> where <span class="math notranslate nohighlight">\( i = 1, 2, \ldots, n \)</span>, the kernel regression estimate of <span class="math notranslate nohighlight">\( y \)</span> at a point <span class="math notranslate nohighlight">\( x \)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[ \hat{y}(x) = \frac{\sum_{i=1}^n K\left(\frac{x - x_i}{h}\right) y_i}{\sum_{i=1}^n K\left(\frac{x - x_i}{h}\right)} \]</div>
<p>Here:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( K(\cdot) \)</span> is the kernel function.</p></li>
<li><p><span class="math notranslate nohighlight">\( h \)</span> is the bandwidth parameter that controls the smoothness of the estimate.</p></li>
</ul>
</section>
<section id="kernel-functions">
<h4>Kernel Functions<a class="headerlink" href="#kernel-functions" title="Link to this heading">#</a></h4>
<p>Commonly used kernel functions include:</p>
<ul class="simple">
<li><p><strong>Gaussian Kernel</strong>: <span class="math notranslate nohighlight">\( K(u) = \frac{1}{\sqrt{2\pi}} e^{-0.5u^2} \)</span></p></li>
<li><p><strong>Epanechnikov Kernel</strong>: <span class="math notranslate nohighlight">\( K(u) = \frac{3}{4}(1 - u^2) \)</span> for <span class="math notranslate nohighlight">\( |u| \leq 1 \)</span></p></li>
<li><p><strong>Uniform Kernel</strong>: <span class="math notranslate nohighlight">\( K(u) = \frac{1}{2} \)</span> for <span class="math notranslate nohighlight">\( |u| \leq 1 \)</span></p></li>
</ul>
<p>The choice of kernel function typically has a minor effect compared to the choice of bandwidth <span class="math notranslate nohighlight">\( h \)</span>.</p>
</section>
<section id="bandwidth-selection">
<h4>Bandwidth Selection<a class="headerlink" href="#bandwidth-selection" title="Link to this heading">#</a></h4>
<p>The bandwidth parameter <span class="math notranslate nohighlight">\( h \)</span> is crucial for kernel regression as it determines the width of the window used to compute the local average. A small <span class="math notranslate nohighlight">\( h \)</span> captures more detail (lower bias but higher variance), while a large <span class="math notranslate nohighlight">\( h \)</span> provides a smoother estimate (higher bias but lower variance). Techniques such as cross-validation are often used to select an appropriate <span class="math notranslate nohighlight">\( h \)</span>.</p>
</section>
<section id="advantages-and-disadvantages">
<h4>Advantages and Disadvantages<a class="headerlink" href="#advantages-and-disadvantages" title="Link to this heading">#</a></h4>
<p><strong>Advantages:</strong></p>
<ul class="simple">
<li><p>No assumption about the underlying data distribution.</p></li>
<li><p>Can model complex, nonlinear relationships.</p></li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul class="simple">
<li><p>Computationally intensive, especially for large datasets.</p></li>
<li><p>Choice of bandwidth is critical and can be challenging.</p></li>
<li><p>Suffer from the curse of dimensionality when extended to higher dimensions.</p></li>
</ul>
</section>
<section id="practical-applications">
<h4>Practical Applications<a class="headerlink" href="#practical-applications" title="Link to this heading">#</a></h4>
<p>Kernel regression is used in various fields, including:</p>
<ul class="simple">
<li><p><strong>Econometrics</strong>: To estimate demand and supply curves.</p></li>
<li><p><strong>Bioinformatics</strong>: For smoothing gene expression data.</p></li>
<li><p><strong>Finance</strong>: To predict stock prices based on historical data.</p></li>
</ul>
</section>
<section id="code-example">
<h4>Code Example<a class="headerlink" href="#code-example" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">gaussian_kernel</span><span class="p">(</span><span class="n">u</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">u</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">kernel_regression</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_query</span><span class="p">,</span> <span class="n">bandwidth</span><span class="p">):</span>
    <span class="n">y_query</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x_query</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x_q</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x_query</span><span class="p">):</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">gaussian_kernel</span><span class="p">((</span><span class="n">x_q</span> <span class="o">-</span> <span class="n">x_train</span><span class="p">)</span> <span class="o">/</span> <span class="n">bandwidth</span><span class="p">)</span>
        <span class="n">y_query</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span> <span class="o">*</span> <span class="n">y_train</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y_query</span>

<span class="c1"># Generate synthetic data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Query points</span>
<span class="n">x_query</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Estimate y values using kernel regression</span>
<span class="n">bandwidth</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">y_query</span> <span class="o">=</span> <span class="n">kernel_regression</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_query</span><span class="p">,</span> <span class="n">bandwidth</span><span class="p">)</span>

<span class="c1"># Plot the results</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_query</span><span class="p">,</span> <span class="n">y_query</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Kernel Regression Estimate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/1fa9e4bb3573dc2a43805ab30a8411ce72c11cfe5b6e28f640b73cc258e11fe2.png" src="../../../_images/1fa9e4bb3573dc2a43805ab30a8411ce72c11cfe5b6e28f640b73cc258e11fe2.png" />
</div>
</div>
</section>
</section>
</section>
<section id="gaussian-process">
<h2>Gaussian Process<a class="headerlink" href="#gaussian-process" title="Link to this heading">#</a></h2>
<p>A Gaussian Process is a powerful non-parametric Bayesian approach, which assumes that any finite set of function values follows a multivariate Gaussian distribution. Abstractly, a Gaussian Process is a probability distribution over the set of functions that can fit a given set of data points.
For start introduce some examples</p>
<section id="example-for-gp">
<h3>Example for GP<a class="headerlink" href="#example-for-gp" title="Link to this heading">#</a></h3>
<p>To express the memory usage of each program as functions of time, let’s denote each program’s memory usage function as <span class="math notranslate nohighlight">\( f_i(x) \)</span>, where <span class="math notranslate nohighlight">\( i \)</span> ranges from 1 to <span class="math notranslate nohighlight">\( n \)</span>. Here, <span class="math notranslate nohighlight">\( x \)</span> represents the time, and each function <span class="math notranslate nohighlight">\( f_i(x) \)</span> represents the memory usage of a specific program at time <span class="math notranslate nohighlight">\( x \)</span>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( f_1(x) \)</span>: Memory usage of Chrome at time <span class="math notranslate nohighlight">\( x \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( f_2(x) \)</span>: Memory usage of Visual Studio Code at time <span class="math notranslate nohighlight">\( x \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( f_3(x) \)</span>: Memory usage of Microsoft Edge at time <span class="math notranslate nohighlight">\( x \)</span></p></li>
</ul>
<p>So, in general:
$<span class="math notranslate nohighlight">\( f_i(x) \)</span><span class="math notranslate nohighlight">\(
represents the memory usage of the \)</span> i <span class="math notranslate nohighlight">\(-th program at time \)</span> x $.</p>
<p>If there are <span class="math notranslate nohighlight">\( n \)</span> programs, we can summarize them as:
$<span class="math notranslate nohighlight">\( \{f_1(x), f_2(x), \ldots, f_n(x)\} \)</span>$</p>
<p>Each function <span class="math notranslate nohighlight">\( f_i(x) \)</span> is a real-valued function defined for all <span class="math notranslate nohighlight">\( x \)</span> in the domain of time, and it describes how the memory usage of the <span class="math notranslate nohighlight">\( i \)</span>-th program changes over time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">psutil</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="k">def</span> <span class="nf">get_memory_usage</span><span class="p">(</span><span class="n">process_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the memory usage of a process by its name.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">proc</span> <span class="ow">in</span> <span class="n">psutil</span><span class="o">.</span><span class="n">process_iter</span><span class="p">([</span><span class="s1">&#39;pid&#39;</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">proc</span><span class="o">.</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="n">process_name</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">process</span> <span class="o">=</span> <span class="n">psutil</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span><span class="n">proc</span><span class="o">.</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;pid&#39;</span><span class="p">])</span>
                <span class="n">memory_info</span> <span class="o">=</span> <span class="n">process</span><span class="o">.</span><span class="n">memory_info</span><span class="p">()</span>
                <span class="k">return</span> <span class="n">memory_info</span><span class="o">.</span><span class="n">rss</span> <span class="o">/</span> <span class="p">(</span><span class="mi">424</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Convert from bytes to MB</span>
            <span class="k">except</span> <span class="p">(</span><span class="n">psutil</span><span class="o">.</span><span class="n">NoSuchProcess</span><span class="p">,</span> <span class="n">psutil</span><span class="o">.</span><span class="n">AccessDenied</span><span class="p">):</span>
                <span class="k">continue</span>
    <span class="k">return</span> <span class="mi">0</span>

<span class="c1"># Define the specific processes to monitor</span>
<span class="n">processes_to_monitor</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;chrome.exe&#39;</span><span class="p">,</span> <span class="s1">&#39;Code.exe&#39;</span><span class="p">,</span> <span class="s1">&#39;msedge.exe&#39;</span><span class="p">]</span>  <span class="c1"># Adjust process names as needed</span>

<span class="c1"># Print the list of processes for verification</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Processes to monitor:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">process</span> <span class="ow">in</span> <span class="n">processes_to_monitor</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">process</span><span class="p">)</span>

<span class="c1"># Define time range and interval</span>
<span class="n">interval</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># Interval in seconds</span>
<span class="n">duration</span> <span class="o">=</span> <span class="mi">300</span>  <span class="c1"># Total duration in seconds</span>

<span class="c1"># Initialize data storage</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Time&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;Process&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;Memory Usage (MB)&#39;</span><span class="p">:</span> <span class="p">[]}</span>

<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="k">while</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">duration</span><span class="p">:</span>
    <span class="n">current_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="k">for</span> <span class="n">process</span> <span class="ow">in</span> <span class="n">processes_to_monitor</span><span class="p">:</span>
        <span class="n">memory_usage</span> <span class="o">=</span> <span class="n">get_memory_usage</span><span class="p">(</span><span class="n">process</span><span class="p">)</span>
        <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Time&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_time</span><span class="p">)</span>
        <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Process&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">process</span><span class="p">)</span>
        <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Memory Usage (MB)&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">memory_usage</span><span class="p">)</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">interval</span><span class="p">)</span>

<span class="c1"># Convert data to a DataFrame</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Plot the memory usage</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="k">for</span> <span class="n">process</span> <span class="ow">in</span> <span class="n">processes_to_monitor</span><span class="p">:</span>
    <span class="n">process_data</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Process&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">process</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">process_data</span><span class="p">[</span><span class="s1">&#39;Time&#39;</span><span class="p">],</span> <span class="n">process_data</span><span class="p">[</span><span class="s1">&#39;Memory Usage (MB)&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">process</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time (seconds)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Memory Usage (MB)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Memory Usage of Selected Processes Over Time&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Processes to monitor:
chrome.exe
Code.exe
msedge.exe
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">line</span> <span class="mi">44</span>
<span class="g g-Whitespace">     </span><span class="mi">42</span>         <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Process&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">process</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">43</span>         <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Memory Usage (MB)&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">memory_usage</span><span class="p">)</span>
<span class="ne">---&gt; </span><span class="mi">44</span>     <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">interval</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">46</span> <span class="c1"># Convert data to a DataFrame</span>
<span class="g g-Whitespace">     </span><span class="mi">47</span> <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
<p>Another example, as shown in the following figure, illustrates vehicles turning left from region A to region B. Each trajectory from A to B is represented by a function <span class="math notranslate nohighlight">\( f_i(x,y) \)</span> for <span class="math notranslate nohighlight">\( i = 1, \ldots, n \)</span>.
<img alt="VehiclesTrajectories" src="../../../_images/VehicleTrajectory.png" /></p>
<p>One of the most applicable areas is stock market patterns, which need to be modeled using Gaussian Processes (GP). Below, you can see a triangular pattern.
<img alt="Triangular Pattern" src="../../../_images/TriangularPattern_StockMarket.png" /></p>
</section>
<section id="regression-from-probability-density-function-perspective">
<h3>Regression from Probability Density Function perspective<a class="headerlink" href="#regression-from-probability-density-function-perspective" title="Link to this heading">#</a></h3>
<p>first, we present the conditional PDF for understanding <span class="math notranslate nohighlight">\( p(y|\theta) \)</span>.</p>
<section id="consider-the-example-p-y-theta-with-a-2-variate-y">
<h4>Consider the example <span class="math notranslate nohighlight">\( p(y|\theta) \)</span> with a 2-variate y<a class="headerlink" href="#consider-the-example-p-y-theta-with-a-2-variate-y" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( p(y_2 | y_1, \theta) \)</span> is the conditional distribution of <span class="math notranslate nohighlight">\( y_2 \)</span> given <span class="math notranslate nohighlight">\( y_1 \)</span> and the parameters <span class="math notranslate nohighlight">\( \theta \)</span> (which includes the covariance matrix <span class="math notranslate nohighlight">\( \Sigma \)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\( p(y_1) \)</span> is the marginal distribution of <span class="math notranslate nohighlight">\( y_1 \)</span>.</p></li>
<li><p>Integrate the joint distribution over <span class="math notranslate nohighlight">\( y_1 \)</span> to find the marginal distribution of <span class="math notranslate nohighlight">\( y \)</span>:
$<span class="math notranslate nohighlight">\(
p(y | \theta) = \int p(y_2 | y_1, \theta) p(y_1) \, dy_1
\)</span><span class="math notranslate nohighlight">\(
Ofcourse \)</span> y_2 <span class="math notranslate nohighlight">\( given \)</span> y_1 <span class="math notranslate nohighlight">\( is also normal:
\)</span><span class="math notranslate nohighlight">\(
y_2 | y_1 \sim \mathcal{N}(\mu_{2|1}, \Sigma_{2|1})
\)</span>$
Where:</p></li>
<li><p><span class="math notranslate nohighlight">\( \mu_{2|1} = \mu_2 + \Sigma_{21} \Sigma_{11}^{-1} (y_1 - \mu_1) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( \Sigma_{2|1} = \Sigma_{22} - \Sigma_{21} \Sigma_{11}^{-1} \Sigma_{12} \)</span></p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\Sigma\)</span> is the covariance matrix partitioned as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Sigma = \begin{pmatrix}
\Sigma_{11} &amp; \Sigma_{12} \\
\Sigma_{21} &amp; \Sigma_{22}
\end{pmatrix}
\end{split}\]</div>
<p>And, The marginal distribution of <span class="math notranslate nohighlight">\( y_1 \)</span> is:
$<span class="math notranslate nohighlight">\(
y_1 \sim \mathcal{N}(\mu_1, \Sigma_{11})
\)</span><span class="math notranslate nohighlight">\(
Finally, integrate to Find \)</span> p(y | \theta) $</p>
<div class="math notranslate nohighlight">
\[
p(y | \theta) = \int p(y_2 | y_1, \theta) p(y_1) \, dy_1
\]</div>
<p>The result of this integration will yield the joint distribution:</p>
<div class="math notranslate nohighlight">
\[
y \sim \mathcal{N}(\mu, \Sigma)
\]</div>
<p>where <span class="math notranslate nohighlight">\( \mu \)</span> and <span class="math notranslate nohighlight">\( \Sigma \)</span> are the mean vector and covariance matrix of the bivariate normal distribution.</p>
</section>
</section>
<section id="visualizing-the-conditional-pdf">
<h3>Visualizing the conditional PDF<a class="headerlink" href="#visualizing-the-conditional-pdf" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the mean and covariance matrix</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># Generate samples from the bivariate normal distribution</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>

<span class="c1"># Define a threshold for y1 to be considered &quot;near&quot; 1</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Find samples where y1 is near 1</span>
<span class="n">near_y1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">threshold</span>

<span class="c1"># Plot the original bivariate samples</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Original Samples&#39;</span><span class="p">)</span>

<span class="c1"># Highlight the samples where y1 is near 1 in red</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="n">near_y1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">samples</span><span class="p">[</span><span class="n">near_y1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Samples near y1=1&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;y1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Samples from Bivariate Normal Distribution&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/3abefd6b997b835bdfe7cdc11ee3411c5fde8ca9872818bc983df34fffa7ae7a.png" src="../../../_images/3abefd6b997b835bdfe7cdc11ee3411c5fde8ca9872818bc983df34fffa7ae7a.png" />
</div>
</div>
<p>Red samples are
$<span class="math notranslate nohighlight">\(
y_2 | y_1 \sim \mathcal{N}(\mu_{2|1}, \Sigma_{2|1})
\)</span><span class="math notranslate nohighlight">\(
All samples are
\)</span><span class="math notranslate nohighlight">\(
y \sim \mathcal{N}(\mu, \Sigma)
\)</span>$</p>
<section id="toward-an-understanding-of-the-process">
<h4>Toward an understanding of the process<a class="headerlink" href="#toward-an-understanding-of-the-process" title="Link to this heading">#</a></h4>
<p>Following example try to visualize the observations <span class="math notranslate nohighlight">\( y \sim \mathcal{N}(\mu, \Sigma) \)</span> from the bivariate normal distribution, plot each dimension of the observations against the observation index, with the x-axis representing the index of the observation. In this case, <span class="math notranslate nohighlight">\( y \)</span> is a vector that includes a total of 50 samples. We show these samples by index.</p>
<section id="y-axis-start-stop">
<h5>Y-axis start-stop<a class="headerlink" href="#y-axis-start-stop" title="Link to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the mean and covariance matrix</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># Generate samples from the bivariate normal distribution</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>

<span class="c1"># Plot the first and second dimensions of the observations</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Plot y1 (first dimension) against the observation index</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">),</span> <span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;y1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Observation Index&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;First variable of Observations&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Plot y2 (second dimension) against the observation index</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">),</span> <span class="n">samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;y2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Observation Index&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Second variable of Observations&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/d80b11af0ac08408439ba403cf2198de8a5fa0017cead34ca4427618eca43edf.png" src="../../../_images/d80b11af0ac08408439ba403cf2198de8a5fa0017cead34ca4427618eca43edf.png" />
</div>
</div>
</section>
</section>
<section id="another-representation">
<h4>Another Representation<a class="headerlink" href="#another-representation" title="Link to this heading">#</a></h4>
<p>A two-variable observation includes only the start and stop points along the y-axis, as shown in the following example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the mean and covariance matrix</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># Generate samples from the bivariate normal distribution</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>

<span class="c1"># Plot the first and second dimensions of the observations</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Plot each observation as a line</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">samples</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Observation </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Dimension Index&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Observations from Bivariate Normal Distribution&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;y1&#39;</span><span class="p">,</span> <span class="s1">&#39;y2&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/67a060ec9fe3f6d53ce5a4baed13580509ca2641690a803a8d5dc9e30da8b337.png" src="../../../_images/67a060ec9fe3f6d53ce5a4baed13580509ca2641690a803a8d5dc9e30da8b337.png" />
</div>
</div>
</section>
<section id="trajectories-as-a-process">
<h4>Trajectories as a Process<a class="headerlink" href="#trajectories-as-a-process" title="Link to this heading">#</a></h4>
<p>A 2D observation consists of two sequences of observations, such as the start and stop points of the trajectories of a vehicle traveling from area A to area B. Each of the 50 trajectories has a start and end point, representing a function. We assume these functions have samples that follow normal distributions. If we have a 4-point trajectory, we must assume that <span class="math notranslate nohighlight">\( y \)</span> is a 4-dimensional vector. Note: These 4 points represent one function out of 50 samples of the trajectory.</p>
<p>First, in the following example, the X and Y axes include 5 sample start points <span class="math notranslate nohighlight">\([(x_1, y_1), \ldots, (x_5, y_5)]\)</span> and 5 sample end points <span class="math notranslate nohighlight">\([(x_1, y_1), \ldots, (x_5, y_5)]\)</span>.</p>
<section id="x-y-plane-including-the-start-and-stop-of-the-function">
<h5>X-Y plane (including the start and stop of the function)<a class="headerlink" href="#x-y-plane-including-the-start-and-stop-of-the-function" title="Link to this heading">#</a></h5>
<p>We need to visualize num_samples trajectories on a 2D plane where each trajectory has a start (x, y) and end (x, y) point, and each trajectory is represented by a line connecting the start and end points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the mean and covariance matrix</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># Generate samples from the bivariate normal distribution</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">start_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>
<span class="n">end_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>

<span class="c1"># Plot the trajectories</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># Plot each trajectory as a line</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">start_points</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">end_points</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="n">start_points</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">end_points</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>

<span class="c1"># Adding labels and titles</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X Position&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Y Position&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Trajectories of 2D Observations from Bivariate Normal Distribution&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/cd37cb30f7b8de9809409d99d6cbcca4f9d821c2c8d0ef4617f5154e7c132bf5.png" src="../../../_images/cd37cb30f7b8de9809409d99d6cbcca4f9d821c2c8d0ef4617f5154e7c132bf5.png" />
</div>
</div>
</section>
<section id="x-y-plane-including-num-points-per-trajectory-of-the-function">
<h5>X-Y plane (including num_points_per_trajectory of the function)<a class="headerlink" href="#x-y-plane-including-num-points-per-trajectory-of-the-function" title="Link to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the mean and covariance matrix for 10 dimensions</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">covariance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="mf">0.8</span><span class="p">)</span>  <span class="c1"># Off-diagonal covariance</span>
<span class="n">np</span><span class="o">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="n">covariance</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>      <span class="c1"># Diagonal covariance</span>

<span class="c1"># Generate trajectories: 50 trajectories each with 10 points in 10D</span>
<span class="n">num_trajectories</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">num_points_per_trajectory</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">trajectories</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">covariance</span><span class="p">,</span> <span class="p">(</span><span class="n">num_trajectories</span><span class="p">,</span> <span class="n">num_points_per_trajectory</span><span class="p">))</span>

<span class="c1"># Define the pair of dimensions to visualize (e.g., dimension 0 and 1)</span>
<span class="n">dim1</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">dim2</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Plotting trajectories</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_trajectories</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">trajectories</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="p">:,</span> <span class="n">dim1</span><span class="p">],</span> <span class="n">trajectories</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="p">:,</span> <span class="n">dim2</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Dimension </span><span class="si">{</span><span class="n">dim1</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Dimension </span><span class="si">{</span><span class="n">dim2</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Trajectories in Dimensions </span><span class="si">{</span><span class="n">dim1</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1"> and </span><span class="si">{</span><span class="n">dim2</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/9970bb58611389026fbada2a4b54ae62e70a72e41e030526884b025c06f69d2f.png" src="../../../_images/9970bb58611389026fbada2a4b54ae62e70a72e41e030526884b025c06f69d2f.png" />
</div>
</div>
<p>We continue to receive the concept of trajectories with a y-axis that has 4 points.</p>
</section>
<section id="y-axis-4-points">
<h5>Y-axis 4-points<a class="headerlink" href="#y-axis-4-points" title="Link to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the mean and covariance matrix for 4 dimensions</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">])</span>  <span class="c1"># Mean vector with y1 = 1, y2 = 2, y3 = 3, and y4 = 2.5</span>
<span class="n">covariance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="mf">0.98</span><span class="p">)</span>  <span class="c1"># Off-diagonal covariance</span>
<span class="n">np</span><span class="o">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="n">covariance</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Diagonal covariance</span>

<span class="c1"># Parameters</span>
<span class="n">num_trajectories</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># Number of trajectories to generate</span>
<span class="n">num_points_per_trajectory</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># Generate samples</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">covariance</span><span class="p">,</span> <span class="n">num_trajectories</span><span class="p">)</span>

<span class="c1"># Indices for the dimensions we are interested in</span>
<span class="n">dim_y1</span><span class="p">,</span> <span class="n">dim_y2</span><span class="p">,</span> <span class="n">dim_y3</span><span class="p">,</span> <span class="n">dim_y4</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span>

<span class="c1"># Define fixed values for y1, y2, y3</span>
<span class="n">fixed_y1</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">fixed_y2</span> <span class="o">=</span> <span class="mf">1.5</span>
<span class="n">fixed_y3</span> <span class="o">=</span> <span class="mf">3.3</span>

<span class="c1"># Define a threshold to select trajectories where y1 is close to fixed_y1 and y4 is close to fixed_y4</span>
<span class="n">threshold_y1</span> <span class="o">=</span> <span class="mf">0.2</span>  <span class="c1"># Range around fixed_y1</span>
<span class="n">fixed_y4</span> <span class="o">=</span> <span class="mf">2.5</span>
<span class="n">threshold_y4</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># Range around fixed_y4</span>

<span class="c1"># Find trajectories where y1 is close to the fixed value and y4 is close to fixed_y4</span>
<span class="n">selected_trajectories</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">selected_trajectories</span><span class="p">)</span><span class="o">&lt;=</span><span class="mi">8</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="n">dim_y1</span><span class="p">]</span> <span class="o">-</span> <span class="n">fixed_y1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">threshold_y1</span> <span class="ow">and</span>
            <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="n">dim_y2</span><span class="p">]</span> <span class="o">-</span> <span class="n">fixed_y2</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">threshold_y1</span> <span class="ow">and</span>
            <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="n">dim_y3</span><span class="p">]</span> <span class="o">-</span> <span class="n">fixed_y3</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">threshold_y1</span> <span class="ow">and</span>
            <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="n">dim_y4</span><span class="p">]</span> <span class="o">-</span> <span class="n">fixed_y4</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">threshold_y4</span><span class="p">):</span>
            <span class="n">selected_trajectories</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>

<span class="c1"># Convert to numpy array for plotting</span>
<span class="n">selected_trajectories</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">selected_trajectories</span><span class="p">)</span>

<span class="c1"># Check if we have enough selected trajectories</span>
<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">selected_trajectories</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;No trajectories match the fixed values. Adjusting the thresholds or mean may be necessary.&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Create figure</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

    <span class="c1"># Plot each trajectory</span>
    <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">selected_trajectories</span><span class="p">:</span>
        <span class="n">y_trajectory</span> <span class="o">=</span> <span class="p">[</span><span class="n">sample</span><span class="p">[</span><span class="n">dim_y1</span><span class="p">],</span> <span class="n">sample</span><span class="p">[</span><span class="n">dim_y2</span><span class="p">],</span> <span class="n">sample</span><span class="p">[</span><span class="n">dim_y3</span><span class="p">],</span> <span class="n">sample</span><span class="p">[</span><span class="n">dim_y4</span><span class="p">]]</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_points_per_trajectory</span><span class="p">),</span> <span class="n">y_trajectory</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Point Index (1 to 4)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Trajectories with $y_1 </span><span class="se">\\</span><span class="s1">approx </span><span class="si">{</span><span class="n">fixed_y1</span><span class="si">}</span><span class="s1">$, $y_2 </span><span class="se">\\</span><span class="s1">approx </span><span class="si">{</span><span class="n">fixed_y2</span><span class="si">}</span><span class="s1">$, $y_3 </span><span class="se">\\</span><span class="s1">approx </span><span class="si">{</span><span class="n">fixed_y3</span><span class="si">}</span><span class="s1">$ and $y_4 </span><span class="se">\\</span><span class="s1">approx </span><span class="si">{</span><span class="n">fixed_y4</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/eba611e2388057ba1400fd3f51c77204778212e5cb68230eb8c05f5c82ad7444.png" src="../../../_images/eba611e2388057ba1400fd3f51c77204778212e5cb68230eb8c05f5c82ad7444.png" />
</div>
</div>
</section>
</section>
</section>
<section id="example-x-y-plane-k-points">
<h3>Example X-Y plane k-points<a class="headerlink" href="#example-x-y-plane-k-points" title="Link to this heading">#</a></h3>
<p>Following code presents motion in X-Y plane including k-points (k=5). Checking out this site is worth your while: <a class="reference external" href="https://thegradient.pub/gaussian-process-not-quite-for-dummies/">https://thegradient.pub/gaussian-process-not-quite-for-dummies/</a></p>
<p>Each sample from the Gaussian process (GP) indeed looks like a smooth wave, which can resemble a sine wave or other smooth functions. This behavior is due to the properties of the Gaussian process and the chosen kernel (RBF in this case).</p>
<section id="rbf-kernel-squared-exponential-kernel">
<h4>RBF Kernel (Squared Exponential Kernel)<a class="headerlink" href="#rbf-kernel-squared-exponential-kernel" title="Link to this heading">#</a></h4>
<p>The RBF kernel (also known as the squared exponential kernel or Gaussian kernel) is defined as:</p>
<div class="math notranslate nohighlight">
\[ k(x_i, x_j) = \sigma_f^2 \exp\left(-\frac{(x_i - x_j)^2}{2\ell^2}\right) \]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \sigma_f^2 \)</span> is the signal variance.</p></li>
<li><p><span class="math notranslate nohighlight">\( \ell \)</span> is the length scale.</p></li>
<li><p><span class="math notranslate nohighlight">\( x_i \)</span> and <span class="math notranslate nohighlight">\( x_j \)</span> are input points.</p></li>
</ul>
<p>This kernel ensures that points close to each other in the input space have highly correlated function values, leading to smooth functions.</p>
</section>
<section id="generating-samples">
<h4>Generating Samples<a class="headerlink" href="#generating-samples" title="Link to this heading">#</a></h4>
<p>When we sample from a multivariate normal distribution with the mean vector and covariance matrix defined by the GP, we get functions that are realizations from the GP prior. Due to the RBF kernel, these functions are smooth and continuous.</p>
</section>
<section id="why-smooth-waves">
<h4>Why Smooth Waves?<a class="headerlink" href="#why-smooth-waves" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Continuity and Smoothness</strong>: The RBF kernel induces smoothness because it enforces strong correlations between nearby points. This results in functions without abrupt changes, making them appear smooth like sine waves.</p></li>
<li><p><strong>Stationarity</strong>: The RBF kernel is stationary, meaning that the smoothness is consistent across the entire input space.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">cdist</span>

<span class="c1"># Define the mean and covariance matrix for num_points_per_trajectory dimensions</span>
<span class="n">num_points_per_trajectory</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_points_per_trajectory</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">rbf_kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">sigma_f</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="n">sqdist</span> <span class="o">=</span> <span class="n">cdist</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="s1">&#39;sqeuclidean&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sigma_f</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">/</span> <span class="n">length_scale</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sqdist</span><span class="p">)</span>

<span class="c1"># Generate sample points</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">num_points_per_trajectory</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Compute the covariance matrix</span>
<span class="n">length_scale</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">sigma_f</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">covariance</span> <span class="o">=</span> <span class="n">rbf_kernel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">sigma_f</span><span class="p">)</span>

<span class="c1"># Add observation noise</span>
<span class="n">sigma_y</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">covariance</span> <span class="o">=</span> <span class="n">covariance</span> <span class="o">+</span> <span class="n">sigma_y</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="c1"># Generate trajectories: 2 trajectories each with num_points_per_trajectory points</span>
<span class="n">num_trajectories</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">trajectories</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">covariance</span><span class="p">,</span> <span class="n">num_trajectories</span><span class="p">)</span>

<span class="c1"># Plotting trajectories</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_trajectories</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">trajectories</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="p">:],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;f(X)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Trajectories of the Gaussian Process&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/a3ee6cac1c66ca06cf4c4c6d42cda8c78ccc9f43d6d57a8d9a9d92c05ffea3af.png" src="../../../_images/a3ee6cac1c66ca06cf4c4c6d42cda8c78ccc9f43d6d57a8d9a9d92c05ffea3af.png" />
</div>
</div>
<p>Now is the time to introduce the relationships of Gaussian Processes from a functional perspective and become familiar with their sampling.</p>
</section>
<section id="define-the-gaussian-process-from-a-functional-perspective">
<h4>Define the Gaussian Process from a functional perspective<a class="headerlink" href="#define-the-gaussian-process-from-a-functional-perspective" title="Link to this heading">#</a></h4>
<p>A Gaussian Process is defined as a collection of random variables, any finite number of which have a joint Gaussian distribution. Formally, a GP is specified by its mean function <span class="math notranslate nohighlight">\( m(x) \)</span> and covariance function (kernel) <span class="math notranslate nohighlight">\( k(x, x') \)</span>:</p>
<div class="math notranslate nohighlight">
\[ f(x) \sim \mathcal{GP}(m(x), k(x, x')) \]</div>
<p>For simplicity, assume that the mean function is zero:</p>
<div class="math notranslate nohighlight">
\[ m(x) = 0 \]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[ f(x) \sim \mathcal{GP}(0, k(x, x')) \]</div>
</section>
<section id="define-the-kernel-function">
<h4>Define the Kernel Function<a class="headerlink" href="#define-the-kernel-function" title="Link to this heading">#</a></h4>
<p>The kernel function <span class="math notranslate nohighlight">\( k(x, x') \)</span> defines the covariance between the function values at two points <span class="math notranslate nohighlight">\( x \)</span> and <span class="math notranslate nohighlight">\( x' \)</span>. A common choice is the Radial Basis Function (RBF) or Gaussian kernel:</p>
<div class="math notranslate nohighlight">
\[ k(x, x') = \sigma_f^2 \exp \left( -\frac{||x - x'||^2}{2\ell^2} \right) \]</div>
<p>Where <span class="math notranslate nohighlight">\( \sigma_f^2 \)</span> represents the signal variance and <span class="math notranslate nohighlight">\( \ell \)</span> denotes the length scale.</p>
<ul class="simple">
<li><p>Vertical scale <span class="math notranslate nohighlight">\( \sigma_f^2 \)</span>: Indicates the range of variation in the function’s values.</p></li>
<li><p>Horizontal scale <span class="math notranslate nohighlight">\( \ell \)</span>: Indicates how rapidly the correlation between two points diminishes as their distance increases. A larger <span class="math notranslate nohighlight">\( \ell \)</span> produces a smoother function, whereas a smaller <span class="math notranslate nohighlight">\( \ell \)</span> results in a more oscillatory function.</p></li>
</ul>
<p><strong>Note:</strong> Hyperparameters <span class="math notranslate nohighlight">\( \ell \)</span> and <span class="math notranslate nohighlight">\( \sigma_f^2 \)</span> can be optimized using maximum likelihood (ML). This process is straightforward because the likelihood is Gaussian.</p>
<div class="math notranslate nohighlight">
\[
\arg\max_{\ell, \sigma_f^2} \, p(y \mid \theta)
\]</div>
</section>
<section id="formulate-the-joint-distribution">
<h4>Formulate the Joint Distribution<a class="headerlink" href="#formulate-the-joint-distribution" title="Link to this heading">#</a></h4>
<p>Given a set of training inputs <span class="math notranslate nohighlight">\(\mathbf{X} = \{x_1, x_2, \ldots, x_N\}\)</span> and corresponding outputs <span class="math notranslate nohighlight">\(\mathbf{y} = \{y_1, y_2, \ldots, y_N\}\)</span>, the prior distribution over the function values <span class="math notranslate nohighlight">\(\mathbf{f} = [f(x_1), f(x_2), \ldots, f(x_N)]^T\)</span> is:</p>
<div class="math notranslate nohighlight">
\[ \mathbf{f} \sim \mathcal{N}(\mathbf{0}, \mathbf{K}) \]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{K}\)</span> is the <span class="math notranslate nohighlight">\(N \times N\)</span> covariance matrix with elements <span class="math notranslate nohighlight">\(K_{ij} = k(x_i, x_j)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">cdist</span>

<span class="c1"># Define the RBF kernel</span>
<span class="k">def</span> <span class="nf">rbf_kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">sigma_f</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="n">sqdist</span> <span class="o">=</span> <span class="n">cdist</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="s1">&#39;sqeuclidean&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sigma_f</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">/</span> <span class="n">length_scale</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sqdist</span><span class="p">)</span>

<span class="c1"># Generate sample points</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Compute the covariance matrix</span>
<span class="n">length_scale</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">sigma_f</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">rbf_kernel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">sigma_f</span><span class="p">)</span>

<span class="c1"># Add observation noise</span>
<span class="n">sigma_y</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">K_noise</span> <span class="o">=</span> <span class="n">K</span> <span class="o">+</span> <span class="n">sigma_y</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="c1"># Draw samples from the GP</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)),</span> <span class="n">K_noise</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>

<span class="c1"># Plot the samples</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">samples</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Samples from the Gaussian Process&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/c0d3bd94d7c2b4ae539f443f653771c4beeaab5fcd397c16e2427cac3d7610ff.png" src="../../../_images/c0d3bd94d7c2b4ae539f443f653771c4beeaab5fcd397c16e2427cac3d7610ff.png" />
</div>
</div>
</section>
</section>
<section id="what-happens-during-sampling">
<h3>What Happens During Sampling<a class="headerlink" href="#what-happens-during-sampling" title="Link to this heading">#</a></h3>
</section>
<section id="multivariate-normal-distribution">
<h3>Multivariate Normal Distribution<a class="headerlink" href="#multivariate-normal-distribution" title="Link to this heading">#</a></h3>
<p>A multivariate normal distribution is defined by a mean vector <span class="math notranslate nohighlight">\(\mu \)</span> and a covariance matrix <span class="math notranslate nohighlight">\(\Sigma \)</span>. For a random vector <span class="math notranslate nohighlight">\(\mathbf{X} \)</span> with <span class="math notranslate nohighlight">\(n \)</span> elements, this can be expressed as:</p>
<div class="math notranslate nohighlight">
\[ \mathbf{X} \sim \mathcal{N}(\mu, \Sigma) \]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu \)</span> is the mean vector of size <span class="math notranslate nohighlight">\(n \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma \)</span> is the <span class="math notranslate nohighlight">\(n \times n \)</span> covariance matrix.</p></li>
</ul>
</section>
<section id="generating-correlated-samples">
<h3>Generating Correlated Samples<a class="headerlink" href="#generating-correlated-samples" title="Link to this heading">#</a></h3>
<p>The covariance matrix <span class="math notranslate nohighlight">\(\Sigma \)</span> encodes the pairwise covariances between the elements of <span class="math notranslate nohighlight">\(\mathbf{X} \)</span>. This means that the elements of the sampled vectors are not independent but rather exhibit correlations as specified by <span class="math notranslate nohighlight">\(\Sigma \)</span>.</p>
<section id="sampling-process">
<h4>Sampling Process<a class="headerlink" href="#sampling-process" title="Link to this heading">#</a></h4>
<p>How samples are generated:</p>
<p><strong>Eigenvalue Decomposition</strong>: The covariance matrix <span class="math notranslate nohighlight">\(\Sigma \)</span> is decomposed into eigenvalues and eigenvectors.</p>
<div class="math notranslate nohighlight">
\[ \Sigma = Q \Lambda Q^T \]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Q \)</span> is an orthogonal matrix of eigenvectors.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Lambda \)</span> is a diagonal matrix of eigenvalues.</p></li>
</ul>
<p><strong>Cholesky Decomposition</strong>: Alternatively, a more common approach for sampling is using Cholesky decomposition.</p>
<div class="math notranslate nohighlight">
\[ \Sigma = LL^T \]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L \)</span> is a lower triangular matrix.</p></li>
</ul>
<p><strong>Generate Standard Normal Samples</strong>: Generate a vector <span class="math notranslate nohighlight">\(\mathbf{z} \)</span> of independent standard normal variables.</p>
<div class="math notranslate nohighlight">
\[ \mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \]</div>
<p><strong>Transform to Desired Distribution</strong>: Transform the standard normal samples to follow the desired covariance structure using the decomposition (Cholesky or eigenvalue).</p>
<div class="math notranslate nohighlight">
\[ \mathbf{x} = \mu + L \mathbf{z} \]</div>
<p>This transformation ensures that the generated samples <span class="math notranslate nohighlight">\(\mathbf{x} \)</span> have the desired mean <span class="math notranslate nohighlight">\(\mu \)</span> and covariance <span class="math notranslate nohighlight">\(\Sigma \)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">cdist</span>

<span class="c1"># Define the RBF kernel</span>
<span class="k">def</span> <span class="nf">rbf_kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">sigma_f</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="n">sqdist</span> <span class="o">=</span> <span class="n">cdist</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="s1">&#39;sqeuclidean&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sigma_f</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">/</span> <span class="n">length_scale</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sqdist</span><span class="p">)</span>

<span class="c1"># Generate sample points</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Compute the covariance matrix</span>
<span class="n">length_scale</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">sigma_f</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">rbf_kernel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">sigma_f</span><span class="p">)</span>

<span class="c1"># Add observation noise</span>
<span class="n">sigma_y</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">K_noise</span> <span class="o">=</span> <span class="n">K</span> <span class="o">+</span> <span class="n">sigma_y</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="c1"># Number of samples to draw</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># Step-by-step sampling process using Cholesky decomposition</span>

<span class="c1"># Mean vector (zeros)</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="c1"># Cholesky decomposition of the covariance matrix</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">K_noise</span><span class="p">)</span>

<span class="c1"># Draw samples from standard normal distribution</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)))</span>

<span class="c1"># Transform to the desired distribution</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">mean</span> <span class="o">+</span> <span class="n">z</span> <span class="o">@</span> <span class="n">L</span><span class="o">.</span><span class="n">T</span>

<span class="c1"># Plot the samples</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">samples</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Samples from the Gaussian Process&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/71dbc741df5b026c15595697b51b6ce42d8bcf0c44ad2221325276bd18cf52bf.png" src="../../../_images/71dbc741df5b026c15595697b51b6ce42d8bcf0c44ad2221325276bd18cf52bf.png" />
</div>
</div>
</section>
</section>
<section id="generate-process-using-quantile-based-distance">
<h3>Generate process using quantile based distance<a class="headerlink" href="#generate-process-using-quantile-based-distance" title="Link to this heading">#</a></h3>
<p>The use of the quantile-based distance function in the computation of the covariance matrix has a notable impact on the Gaussian Process (GP) modeling and the generation of samples.</p>
<p><strong>Effect on Covariance Matrix</strong></p>
<p>The covariance matrix (or kernel matrix) in Gaussian Processes defines how the observations are related to each other. The choice of distance metric in the kernel function affects this covariance structure.</p>
<section id="traditional-rbf-kernel">
<h4><strong>Traditional RBF Kernel:</strong><a class="headerlink" href="#traditional-rbf-kernel" title="Link to this heading">#</a></h4>
<p>For a traditional Radial Basis Function (RBF) kernel, the covariance between two points <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(x_j\)</span> is computed as:</p>
<div class="math notranslate nohighlight">
\[
K(x_i, x_j) = \sigma_f^2 \exp\left(-\frac{\|x_i - x_j\|^2}{2 l^2}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\|x_i - x_j\|^2\)</span> is the squared Euclidean distance between <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(x_j\)</span>, <span class="math notranslate nohighlight">\( \sigma_f \)</span> is the signal variance, and <span class="math notranslate nohighlight">\( l \)</span> is the length scale.</p>
</section>
<section id="quantile-based-distance">
<h4><strong>Quantile-Based Distance:</strong><a class="headerlink" href="#quantile-based-distance" title="Link to this heading">#</a></h4>
<p>When you replace the squared Euclidean distance with a quantile-based distance (as derived from the quantile loss function), the covariance matrix becomes:</p>
<div class="math notranslate nohighlight">
\[
K_{ij} = \sigma_f^2 \exp\left(-\frac{ \text{quantile\_loss}(x_i, x_j)^2}{2 l^2}\right)
\]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[
\text{quantile\_loss}(x_i, x_j) = \text{max}\left((\tau - 1) \cdot (x_i - x_j), \tau \cdot (x_j - x_i)\right)
\]</div>
</section>
<section id="impact-of-quantile-based-distance">
<h4><strong>Impact of Quantile-Based Distance:</strong><a class="headerlink" href="#impact-of-quantile-based-distance" title="Link to this heading">#</a></h4>
<p><strong>Sensitivity to Outliers</strong>:</p>
<ul class="simple">
<li><p><strong>Traditional RBF</strong>: The RBF kernel smooths distances based on the squared Euclidean distance, which can be sensitive to outliers or large deviations.</p></li>
<li><p><strong>Quantile-Based Distance</strong>: By focusing on the quantile of errors, the kernel can handle deviations differently, potentially reducing sensitivity to outliers or extreme values depending on the chosen quantile.</p></li>
</ul>
<p><strong>Distance Sensitivity</strong>:</p>
<ul class="simple">
<li><p><strong>Traditional RBF</strong>: Distance is computed in a straightforward manner; points that are far apart will have exponentially decreasing covariance.</p></li>
<li><p><strong>Quantile-Based Distance</strong>: The distance is influenced by the quantile-specific loss, which may lead to non-standard distances that reflect the prediction error more robustly. For different quantile values, the distance metric becomes sensitive to different aspects of prediction errors.</p></li>
</ul>
<p><strong>Covariance Structure</strong>:</p>
<ul class="simple">
<li><p><strong>Traditional RBF</strong>: The covariance matrix is typically smooth and positive definite.</p></li>
<li><p><strong>Quantile-Based Distance</strong>: The covariance matrix might exhibit different structure, particularly with varying quantiles. For higher quantile values, the covariance might become more concentrated, emphasizing large deviations more.</p></li>
</ul>
<p><strong>Effect on Sample Generation</strong></p>
<p>When using the quantile-based distance in the covariance matrix, the generated samples from the GP will reflect the altered covariance structure. This affects the generated functions’ appearance and behavior.</p>
</section>
<section id="sample-generation-with-traditional-rbf-kernel">
<h4><strong>Sample Generation with Traditional RBF Kernel:</strong><a class="headerlink" href="#sample-generation-with-traditional-rbf-kernel" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Smoothness</strong>: Functions are generally smooth due to the exponential decay of covariance with distance.</p></li>
<li><p><strong>Variability</strong>: Functions will vary smoothly according to the chosen length scale and signal variance.</p></li>
</ul>
<p><strong>Higher Quantile Values</strong>:If the quantile is higher (e.g., 0.9), the distance emphasizes positive errors (giving more weight to positive errors).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">cdist</span>


<span class="k">def</span> <span class="nf">quantile_loss</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">quantile</span><span class="p">):</span>
    <span class="c1">#quantile_losses = []</span>
    <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">quantile</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Quantile should be in (0, 1) range&quot;</span>
    <span class="n">n1</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
    <span class="n">n2</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n1</span><span class="p">,</span> <span class="n">n2</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n2</span><span class="p">):</span>
            <span class="n">errors</span> <span class="o">=</span> <span class="n">target</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">preds</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">((</span><span class="n">quantile</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">errors</span><span class="p">,</span> <span class="n">quantile</span> <span class="o">*</span> <span class="n">errors</span><span class="p">)</span>
            <span class="n">K</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">=</span><span class="mi">4</span><span class="o">*</span><span class="n">loss</span><span class="o">**</span><span class="mi">2</span>
            <span class="c1">#quantile_losses.append(loss)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">K</span><span class="p">)</span>
    
<span class="c1"># Define the RBF kernel</span>
<span class="k">def</span> <span class="nf">rbf_kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">sigma_f</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">quantile</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="n">sqdist</span> <span class="o">=</span> <span class="n">quantile_loss</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">quantile</span><span class="p">)</span>
    
    <span class="c1">#sqdist = cdist(x1, x2, &#39;sqeuclidean&#39;)</span>
    
    <span class="k">return</span> <span class="n">sigma_f</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">/</span> <span class="n">length_scale</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sqdist</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">GenSamples</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">sigma_f</span><span class="p">,</span> <span class="n">quantile3</span><span class="p">):</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">rbf_kernel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">sigma_f</span><span class="p">,</span> <span class="n">quantile3</span><span class="p">)</span>
    <span class="c1"># Add observation noise</span>
    <span class="n">sigma_y</span> <span class="o">=</span> <span class="mf">0.01</span>
    <span class="n">K_noise</span> <span class="o">=</span> <span class="n">K</span> <span class="c1">#+ sigma_y**2 * np.eye(len(X))</span>
    <span class="c1"># Draw samples from the GP</span>
    <span class="c1"># Set a seed for reproducibility</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">samples3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)),</span> <span class="n">K_noise</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">samples3</span>

<span class="c1"># Generate sample points</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Compute the covariance matrix</span>
<span class="n">length_scale</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">sigma_f</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">quantile1</span> <span class="o">=</span><span class="mf">0.9</span>
<span class="n">samples1</span> <span class="o">=</span><span class="n">GenSamples</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">sigma_f</span><span class="p">,</span> <span class="n">quantile1</span><span class="p">)</span>

<span class="n">quantile2</span> <span class="o">=</span><span class="mf">0.7</span>
<span class="n">samples2</span> <span class="o">=</span><span class="n">GenSamples</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">sigma_f</span><span class="p">,</span> <span class="n">quantile2</span><span class="p">)</span>

<span class="n">quantile3</span> <span class="o">=</span><span class="mf">0.6</span>
<span class="n">samples3</span> <span class="o">=</span><span class="n">GenSamples</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">sigma_f</span><span class="p">,</span> <span class="n">quantile3</span><span class="p">)</span>

<span class="n">quantile4</span> <span class="o">=</span><span class="mf">0.5</span>
<span class="n">samples4</span> <span class="o">=</span><span class="n">GenSamples</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">sigma_f</span><span class="p">,</span> <span class="n">quantile4</span><span class="p">)</span>

<span class="n">quantile5</span> <span class="o">=</span><span class="mf">0.1</span>
<span class="n">samples5</span> <span class="o">=</span><span class="n">GenSamples</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">sigma_f</span><span class="p">,</span> <span class="n">quantile5</span><span class="p">)</span>

<span class="c1"># Plot the samples</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">samples1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Quantile no1 (Quantile1=</span><span class="si">{</span><span class="n">quantile1</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">10</span><span class="p">],</span> <span class="n">samples1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)[</span><span class="mi">10</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;quantile1: </span><span class="si">{</span><span class="n">quantile1</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">samples2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Quantile no2 (Quantile1=</span><span class="si">{</span><span class="n">quantile2</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">25</span><span class="p">],</span> <span class="n">samples2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)[</span><span class="mi">25</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;quantile2: </span><span class="si">{</span><span class="n">quantile2</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">samples3</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Quantile no3 (Quantile3=</span><span class="si">{</span><span class="n">quantile3</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">50</span><span class="p">],</span> <span class="n">samples3</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)[</span><span class="mi">50</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;quantile3: </span><span class="si">{</span><span class="n">quantile3</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">samples4</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;cyan&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Quantile no4 (Quantile4=</span><span class="si">{</span><span class="n">quantile4</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">60</span><span class="p">],</span> <span class="n">samples4</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)[</span><span class="mi">60</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;quantile4: </span><span class="si">{</span><span class="n">quantile4</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;cyan&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">samples5</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Quantile no5 (Quantile5=</span><span class="si">{</span><span class="n">quantile5</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">70</span><span class="p">],</span> <span class="n">samples5</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)[</span><span class="mi">70</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;quantile5: </span><span class="si">{</span><span class="n">quantile5</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>



<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Samples from the Gaussian Process&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\Dr\AppData\Local\Temp\ipykernel_13692\3341258912.py:17: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
  K[i,j]=4*loss**2
C:\Users\Dr\AppData\Local\Temp\ipykernel_13692\3341258912.py:38: RuntimeWarning: covariance is not symmetric positive-semidefinite.
  samples3 = np.random.multivariate_normal(np.zeros(len(X)), K_noise, num_samples)
</pre></div>
</div>
<img alt="../../../_images/f62533f065f66b7f7750cb3d1ab0aa3379bc681180040352038659c5861dd6ff.png" src="../../../_images/f62533f065f66b7f7750cb3d1ab0aa3379bc681180040352038659c5861dd6ff.png" />
</div>
</div>
</section>
</section>
<section id="explanation-of-epsilon-insensitive-loss-for-process-generation">
<h3>Explanation of <span class="math notranslate nohighlight">\(\epsilon\)</span>-Insensitive Loss for Process Generation<a class="headerlink" href="#explanation-of-epsilon-insensitive-loss-for-process-generation" title="Link to this heading">#</a></h3>
<p>The <span class="math notranslate nohighlight">\(\epsilon\)</span>-insensitive loss is used to define a distance measure that only considers errors (or deviations) larger than a specified threshold <span class="math notranslate nohighlight">\(\epsilon\)</span>. This method is particularly useful in support vector regression (SVR) and can also be adapted to Gaussian processes to model functions that tolerate small deviations without penalty.
The <span class="math notranslate nohighlight">\(\epsilon\)</span>-insensitive loss function is defined as:</p>
<div class="math notranslate nohighlight">
\[
L_{\epsilon}(y_i, y_j) = \max(0, |y_i - y_j| - \epsilon)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span> is a hyperparameter that defines the insensitivity range. If the absolute difference <span class="math notranslate nohighlight">\(|y_i - y_j|\)</span> is less than <span class="math notranslate nohighlight">\(\epsilon\)</span>, the loss is zero; otherwise, it is the amount by which the difference exceeds <span class="math notranslate nohighlight">\(\epsilon\)</span>.</p>
<p><strong>Formulate the Distance Metric:</strong>
Square the <span class="math notranslate nohighlight">\(\epsilon\)</span>-insensitive loss to form a distance metric:</p>
<div class="math notranslate nohighlight">
\[
d_{\epsilon}(x_i, x_j) = 4 \cdot \left(\max(0, |y_i - y_j| - \epsilon)\right)^2
\]</div>
<p>This distance metric will be used in place of the standard Euclidean distance in the RBF kernel.</p>
<p><strong>Compute the Covariance Matrix:</strong>
Using the distance metric, compute the covariance matrix with the RBF kernel:</p>
<div class="math notranslate nohighlight">
\[
K_{ij} = \sigma_f^2 \exp\left(-\frac{d_{\epsilon}(x_i, x_j)}{2 l^2}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma_f\)</span> is the signal variance, and <span class="math notranslate nohighlight">\(l\)</span> is the length scale.</p>
</section>
<section id="impact-on-process-generation">
<h3>Impact on Process Generation<a class="headerlink" href="#impact-on-process-generation" title="Link to this heading">#</a></h3>
<p><strong>Effect of <span class="math notranslate nohighlight">\(\epsilon\)</span> Value:</strong></p>
<ul class="simple">
<li><p><strong>Small <span class="math notranslate nohighlight">\(\epsilon\)</span>:</strong> When <span class="math notranslate nohighlight">\(\epsilon\)</span> is small, the loss function is more sensitive to small deviations. The resulting distance metric <span class="math notranslate nohighlight">\(d_{\epsilon}(x_i, x_j)\)</span> will be larger for small differences.</p></li>
<li><p><strong>Large <span class="math notranslate nohighlight">\(\epsilon\)</span>:</strong> When <span class="math notranslate nohighlight">\(\epsilon\)</span> is large, small deviations are ignored, and only larger differences contribute to the distance metric.</p></li>
</ul>
<p><strong>Sample Characteristics:</strong></p>
<ul class="simple">
<li><p><strong>Smoothness:</strong> Smaller <span class="math notranslate nohighlight">\(\epsilon\)</span> values lead to smoother samples since the covariance between nearby points is higher.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">cdist</span>

<span class="k">def</span> <span class="nf">epsilon_loss</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="n">n1</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
    <span class="n">n2</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n1</span><span class="p">,</span> <span class="n">n2</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n2</span><span class="p">):</span>
            <span class="n">errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">target</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">preds</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">errors</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>
            <span class="n">K</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">loss</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">K</span>

<span class="k">def</span> <span class="nf">rbf_kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">sigma_f</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="n">sqdist</span> <span class="o">=</span> <span class="n">epsilon_loss</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sigma_f</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">/</span> <span class="n">length_scale</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sqdist</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">GenSamples</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">sigma_f</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">rbf_kernel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">sigma_f</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="n">sigma_y</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">K_noise</span> <span class="o">=</span> <span class="n">K</span> <span class="o">+</span> <span class="n">sigma_y</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)),</span> <span class="n">K_noise</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">samples</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">length_scale</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">sigma_f</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">epsilon1</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">samples1</span> <span class="o">=</span> <span class="n">GenSamples</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">sigma_f</span><span class="p">,</span> <span class="n">epsilon1</span><span class="p">)</span>

<span class="n">epsilon2</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">samples2</span> <span class="o">=</span> <span class="n">GenSamples</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">sigma_f</span><span class="p">,</span> <span class="n">epsilon2</span><span class="p">)</span>

<span class="n">epsilon3</span> <span class="o">=</span> <span class="mf">3.0</span>
<span class="n">samples3</span> <span class="o">=</span> <span class="n">GenSamples</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">sigma_f</span><span class="p">,</span> <span class="n">epsilon3</span><span class="p">)</span>

<span class="n">epsilon4</span> <span class="o">=</span> <span class="mf">5.0</span>
<span class="n">samples4</span> <span class="o">=</span> <span class="n">GenSamples</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">sigma_f</span><span class="p">,</span> <span class="n">epsilon4</span><span class="p">)</span>

<span class="n">epsilon5</span> <span class="o">=</span> <span class="mf">8.0</span>
<span class="n">samples5</span> <span class="o">=</span> <span class="n">GenSamples</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">length_scale</span><span class="p">,</span> <span class="n">sigma_f</span><span class="p">,</span> <span class="n">epsilon5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">samples1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Epsilon no1 (Epsilon=</span><span class="si">{</span><span class="n">epsilon1</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">10</span><span class="p">],</span> <span class="n">samples1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)[</span><span class="mi">10</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;epsilon1: </span><span class="si">{</span><span class="n">epsilon1</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">samples2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Epsilon no2 (Epsilon=</span><span class="si">{</span><span class="n">epsilon2</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">25</span><span class="p">],</span> <span class="n">samples2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)[</span><span class="mi">25</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;epsilon2: </span><span class="si">{</span><span class="n">epsilon2</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">samples3</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Epsilon no3 (Epsilon=</span><span class="si">{</span><span class="n">epsilon3</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">50</span><span class="p">],</span> <span class="n">samples3</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)[</span><span class="mi">50</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;epsilon3: </span><span class="si">{</span><span class="n">epsilon3</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">samples4</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;cyan&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Epsilon no4 (Epsilon=</span><span class="si">{</span><span class="n">epsilon4</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">60</span><span class="p">],</span> <span class="n">samples4</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)[</span><span class="mi">60</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;epsilon4: </span><span class="si">{</span><span class="n">epsilon4</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;cyan&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">samples5</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Epsilon no5 (Epsilon=</span><span class="si">{</span><span class="n">epsilon5</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">70</span><span class="p">],</span> <span class="n">samples5</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)[</span><span class="mi">70</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;epsilon5: </span><span class="si">{</span><span class="n">epsilon5</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Samples from the Gaussian Process with $\epsilon$-Insensitive Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;&gt;:68: SyntaxWarning: invalid escape sequence &#39;\e&#39;
&lt;&gt;:68: SyntaxWarning: invalid escape sequence &#39;\e&#39;
C:\Users\Dr\AppData\Local\Temp\ipykernel_13692\3299196453.py:68: SyntaxWarning: invalid escape sequence &#39;\e&#39;
  plt.title(&#39;Samples from the Gaussian Process with $\epsilon$-Insensitive Loss&#39;)
C:\Users\Dr\AppData\Local\Temp\ipykernel_13692\3299196453.py:14: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
  K[i, j] = 4 * loss**2
C:\Users\Dr\AppData\Local\Temp\ipykernel_13692\3299196453.py:27: RuntimeWarning: covariance is not symmetric positive-semidefinite.
  samples = np.random.multivariate_normal(np.zeros(len(X)), K_noise, num_samples)
</pre></div>
</div>
<img alt="../../../_images/fe77e9f50f85dfc3909c5ef879457e724666dd1df3ec66783a0c5db04be4c842.png" src="../../../_images/fe77e9f50f85dfc3909c5ef879457e724666dd1df3ec66783a0c5db04be4c842.png" />
</div>
</div>
</section>
<section id="mini-project-exploring-smoothness-and-loss-functions-in-gaussian-process-generation">
<h3>Mini Project: Exploring Smoothness and Loss Functions in Gaussian Process Generation<a class="headerlink" href="#mini-project-exploring-smoothness-and-loss-functions-in-gaussian-process-generation" title="Link to this heading">#</a></h3>
<section id="objectives">
<h4>Objectives:<a class="headerlink" href="#objectives" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p>Investigate how different loss functions impact the smoothness and variability of Gaussian Process (GP) generated samples.</p></li>
<li><p>Compare standard distance metrics like squared Euclidean distance with advanced loss functions such as quantile loss and (\epsilon)-insensitive loss.</p></li>
<li><p>Visualize and analyze the generated processes for different loss functions and parameter settings.</p></li>
<li><p>Analyze the impact of each loss function on the smoothness and variability of the generated processes.</p></li>
</ol>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./courses\ML\Regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../../machine_learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Machine Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="KernelRegression.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Kernel Regression</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-specification">Model Specification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#review-of-concepts">Review of concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-model">Regression Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-and-variance-of-the-model">Mean and Variance of the Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-of-the-linear-regression-model">Likelihood of the Linear Regression Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#log-likelihood">Log-Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-for-w">Solving for <span class="math notranslate nohighlight">\( w \)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-for-sigma">Solving for <span class="math notranslate nohighlight">\( \sigma \)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-and-confidence-intervals">Prediction and Confidence Intervals</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-regression">Kernel Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-regression-vs-gaussian-processes">Kernel Regression vs. Gaussian Processes</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Kernel Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#key-points">Key Points:</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-processes">Gaussian Processes</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Key Points:</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-kernel-regression">Introduction to Kernel Regression</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-formulation">Mathematical Formulation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-functions">Kernel Functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bandwidth-selection">Bandwidth Selection</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-and-disadvantages">Advantages and Disadvantages</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-applications">Practical Applications</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#code-example">Code Example</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-process">Gaussian Process</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-for-gp">Example for GP</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-from-probability-density-function-perspective">Regression from Probability Density Function perspective</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#consider-the-example-p-y-theta-with-a-2-variate-y">Consider the example <span class="math notranslate nohighlight">\( p(y|\theta) \)</span> with a 2-variate y</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-conditional-pdf">Visualizing the conditional PDF</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#toward-an-understanding-of-the-process">Toward an understanding of the process</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#y-axis-start-stop">Y-axis start-stop</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#another-representation">Another Representation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#trajectories-as-a-process">Trajectories as a Process</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#x-y-plane-including-the-start-and-stop-of-the-function">X-Y plane (including the start and stop of the function)</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#x-y-plane-including-num-points-per-trajectory-of-the-function">X-Y plane (including num_points_per_trajectory of the function)</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#y-axis-4-points">Y-axis 4-points</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-x-y-plane-k-points">Example X-Y plane k-points</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rbf-kernel-squared-exponential-kernel">RBF Kernel (Squared Exponential Kernel)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-samples">Generating Samples</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-smooth-waves">Why Smooth Waves?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-gaussian-process-from-a-functional-perspective">Define the Gaussian Process from a functional perspective</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-kernel-function">Define the Kernel Function</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#formulate-the-joint-distribution">Formulate the Joint Distribution</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-happens-during-sampling">What Happens During Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-normal-distribution">Multivariate Normal Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-correlated-samples">Generating Correlated Samples</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-process">Sampling Process</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-process-using-quantile-based-distance">Generate process using quantile based distance</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#traditional-rbf-kernel"><strong>Traditional RBF Kernel:</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#quantile-based-distance"><strong>Quantile-Based Distance:</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#impact-of-quantile-based-distance"><strong>Impact of Quantile-Based Distance:</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-generation-with-traditional-rbf-kernel"><strong>Sample Generation with Traditional RBF Kernel:</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-of-epsilon-insensitive-loss-for-process-generation">Explanation of <span class="math notranslate nohighlight">\(\epsilon\)</span>-Insensitive Loss for Process Generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#impact-on-process-generation">Impact on Process Generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mini-project-exploring-smoothness-and-loss-functions-in-gaussian-process-generation">Mini Project: Exploring Smoothness and Loss Functions in Gaussian Process Generation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#objectives">Objectives:</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dr.Hadi Sadoghi Yazdi
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024 Pattern Recognition Lab.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>