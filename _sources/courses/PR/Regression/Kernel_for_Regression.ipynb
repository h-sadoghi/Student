{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We break down the concept of kernel methods in a simple form.\n",
    "\n",
    "## Kernel Methods and the Kernel Trick\n",
    "\n",
    "**Input Space and Feature Space**:\n",
    "   - **Input Space ($ \\mathcal{X} $)**: This is the space where your original data points, $ x $ and $ y $, reside.\n",
    "   - **Feature Space ($ \\mathcal{F} $)**: This is a higher-dimensional space where your data points are mapped using a feature mapping function, $ \\phi $. So, $ \\phi(x) $ and $ \\phi(y) $ are the representations of $ x $ and $ y $ in the feature space.\n",
    "\n",
    "**Feature Mapping ($ \\phi $)**:\n",
    "   - The feature mapping function $ \\phi: \\mathcal{X} \\rightarrow \\mathcal{F} $ transforms data points from the input space to the feature space.\n",
    "   - In many cases, the feature space can be very high-dimensional, even infinite-dimensional, which makes computations directly in $ \\mathcal{F} $ **_impractical_**.\n",
    "\n",
    "**Inner Product in Feature Space**:\n",
    "   - When using machine learning algorithms, we often need to compute the inner product between two points in the feature space: $\\left\\langle \\phi(x) , \\phi(y) \\right\\rangle  $.\n",
    "\n",
    "**Kernel Function ($ k $)**:\n",
    "   - The kernel function $ k $ is defined as $ k(x, y) = \\phi(x)^{T} \\phi(y) $.\n",
    "   - The key idea is that we do not need to compute $ \\phi(x) $ and $ \\phi(y) $ explicitly. Instead, we directly compute $ k(x, y) $ in the input space.\n",
    "\n",
    "<span style=\"color:green\">**The Kernel Trick**</span>\n",
    "\n",
    " The kernel trick allows us to work in the high-dimensional feature space implicitly without ever computing the coordinates of the data points in that space. Instead, we compute the inner products using the kernel function.\n",
    "\n",
    "***Why is this useful?***\n",
    "\n",
    "**Efficiency**:\n",
    "   - Computing $ \\phi(x) $ and $ \\phi(y) $ explicitly can be computationally expensive or infeasible.\n",
    "   - Using the kernel function $ k(x, y) $, we can perform the same calculations much more efficiently.\n",
    "\n",
    "**Flexibility**:\n",
    "   - Kernel methods allow us to use different types of kernels, each corresponding to a different feature space. This flexibility helps in capturing various data structures and relationships.\n",
    "\n",
    "### Common Kernels\n",
    "\n",
    "**Linear Kernel**:\n",
    "   - $ k(x, y) = x^{T} y $\n",
    "   - This corresponds to no mapping (i.e., $ \\phi(x) = x $).\n",
    "\n",
    "**Polynomial Kernel**:\n",
    "   - $ k(x, y) = (x^{T} y + c)^d $\n",
    "   - This corresponds to a polynomial feature mapping.\n",
    "\n",
    "**Gaussian (RBF) Kernel**:\n",
    "   - $ k(x, y) = \\exp \\left( -\\frac{\\|x - y\\|^2}{2\\sigma^2} \\right) $\n",
    "   - This corresponds to mapping into an infinite-dimensional feature space.\n",
    "\n",
    "**Sigmoid Kernel**:\n",
    "   - $ k(x, y) = \\tanh(\\alpha x^{T} y + c) $\n",
    "   - This corresponds to a feature mapping inspired by neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding $ \\phi (x) $ From Kernel\n",
    "\n",
    "_Example 1_: **Polynomial kernel**\n",
    "\n",
    "We try decompose the polynomial kernel $ k(x, y) = (x^T y + c)^2 $ into its feature mapping representation $ \\phi(x) $ such that $ k(x, y) = \\left\\langle \\phi(x) , \\phi(y) \\right\\rangle  $.\n",
    "\n",
    "**_Work Steps_**\n",
    "\n",
    "**Expand the Polynomial Kernel**:\n",
    "\n",
    "   $$\n",
    "   k(x, y) = (x^T y + c)^2\n",
    "   $$\n",
    "\n",
    "   Using the binomial expansion, we get:\n",
    "\n",
    "   $$\n",
    "   (x^T y + c)^2 = (x^T y)^2 + 2c(x^T y) + c^2\n",
    "   $$\n",
    "\n",
    "**Identify the Terms**:\n",
    "   Let's consider $ x = [x_1, x_2, \\ldots, x_d]^T $ and $ y = [y_1, y_2, \\ldots, y_d]^T $. The expanded terms can be further decomposed:\n",
    "\n",
    "   $$\n",
    "   (x^T y)^2 = \\left( \\sum_{i=1}^{d} x_i y_i \\right)^2 = \\sum_{i=1}^{d} \\sum_{j=1}^{d} x_i y_i x_j y_j\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   2c(x^T y) = 2c \\sum_{i=1}^{d} x_i y_i\n",
    "   $$\n",
    "\n",
    "**Feature Mapping ($ \\phi(x) $)**:\n",
    "   To express $ k(x, y) $ as $ \\phi(x)^T \\phi(y) $, we need to identify a feature mapping $ \\phi(x) $ such that the inner product in the feature space gives us the polynomial kernel.\n",
    "\n",
    "   Let's construct $ \\phi(x) $ as follows:\n",
    "\n",
    "   $$\n",
    "   \\phi(x) = \\left( \\sqrt{2}x_1x_1, \\sqrt{2}x_1x_2, \\ldots, \\sqrt{2}x_1x_d, \\sqrt{2}x_2x_1, \\sqrt{2}x_2x_2, \\ldots, \\sqrt{2}x_d x_d, \\sqrt{2c}x_1, \\sqrt{2c}x_2, \\ldots, \\sqrt{2c}x_d, c \\right)^T\n",
    "   $$\n",
    "\n",
    "   Here, $ \\phi(x) $ includes:\n",
    "   - All pairwise product terms $ \\sqrt{2} x_i x_j $.\n",
    "   - Linear terms $ \\sqrt{2c} x_i $.\n",
    "   - A constant term $ c $.\n",
    "\n",
    "**Verify $ \\phi(x)^T \\phi(y) $**:\n",
    "   Now, we compute the inner product $ \\phi(x)^T \\cdot \\phi(y) $:\n",
    "\n",
    "   $$\n",
    "   \\phi(x)^T \\cdot \\phi(y) = \\left( \\sqrt{2}x_1x_1, \\sqrt{2}x_1x_2, \\ldots, \\sqrt{2}x_d x_d, \\sqrt{2c}x_1, \\sqrt{2c}x_2, \\ldots, c \\right)^T \\cdot \\left( \\sqrt{2}y_1y_1, \\sqrt{2}y_1y_2, \\ldots, \\sqrt{2}y_d y_d, \\sqrt{2c}y_1, \\sqrt{2c}y_2, \\ldots, c \\right)\n",
    "   $$\n",
    "\n",
    "   This results in:\n",
    "\n",
    "   $$\n",
    "   \\sum_{i=1}^{d} \\sum_{j=1}^{d} \\sqrt{2} x_i x_j \\sqrt{2} y_i y_j + \\sum_{i=1}^{d} \\sqrt{2c} x_i \\sqrt{2c} y_i + c^2 = 2 \\sum_{i=1}^{d} \\sum_{j=1}^{d} x_i x_j y_i y_j + 2c \\sum_{i=1}^{d} x_i y_i + c^2\n",
    "   $$\n",
    "\n",
    "   Simplifying:\n",
    "   $$\n",
    "   2 (x^T y)^2 + 2c (x^T y) + c^2 = (x^T y + c)^2\n",
    "   $$\n",
    "\n",
    "Thus, we have:\n",
    "\n",
    "$$\n",
    "k(x, y) = (x^T y + c)^2 = \\phi(x)^T \\phi(y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Example 2_: **RBF kernel**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian (RBF) kernel is:\n",
    "\n",
    "$$ \n",
    "k(x, y) = \\exp\\left(-\\frac{\\|x - y\\|^2}{2\\sigma^2}\\right) \n",
    "$$\n",
    "\n",
    "First, let's rewrite the squared Euclidean distance:\n",
    "\n",
    "$$ \n",
    "\\|x - y\\|^2 = x^T x + y^T y - 2 x^T y \n",
    "$$\n",
    "\n",
    "So the kernel can be written as:\n",
    "\n",
    "$$ \n",
    "k(x, y) = \\exp\\left(-\\frac{x^T x + y^T y - 2 x^T y}{2\\sigma^2}\\right) \n",
    "$$\n",
    "\n",
    "This can be further split into:\n",
    "\n",
    "$$ \n",
    "k(x, y) = \\exp\\left(-\\frac{x^T x}{2\\sigma^2}\\right) \\exp\\left(-\\frac{y^T y}{2\\sigma^2}\\right) \\exp\\left(\\frac{x^T y}{\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "## <span style=\"color:blue\">**Homework 1**</span> \n",
    "Decompose the term $ \\exp\\left(\\frac{x^T y}{\\sigma^2}\\right) $ and find $ \\phi(x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods in $ \\phi $ space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance in $ \\phi(x) $ space\n",
    "To compute $\\|\\phi(x) - \\phi(y)\\|^2$ in the feature space and relate it to the Gaussian (RBF) kernel $k(x, y)$, we need to follow these steps:\n",
    "\n",
    "**Define the Kernel Function**:\n",
    "   The Gaussian (RBF) kernel is given by:\n",
    "   \n",
    "   $$\n",
    "   k(x, y) = \\exp\\left(-\\frac{\\|x - y\\|^2}{2\\sigma^2}\\right)\n",
    "   $$\n",
    "\n",
    "**Feature Mapping**:\n",
    "   The Gaussian kernel can be thought of as corresponding to an infinite-dimensional feature space. In this space, the kernel function $k(x, y)$ can be expressed as the inner product of feature vectors $\\phi(x)$ and $\\phi(y)$:\n",
    "   \n",
    "   $$\n",
    "   k(x, y) = \\phi(x)^T \\phi(y)\n",
    "   $$\n",
    "\n",
    "**Compute $\\|\\phi(x) - \\phi(y)\\|^2$**:\n",
    "   To find $\\|\\phi(x) - \\phi(y)\\|^2$, we use the fact that:\n",
    "   $$\n",
    "   \\|\\phi(x) - \\phi(y)\\|^2 = \\|\\phi(x)\\|^2 + \\|\\phi(y)\\|^2 - 2 \\phi(x)^T \\phi(y)\n",
    "   $$\n",
    "\n",
    "   We need to compute $\\|\\phi(x)\\|^2$ and $\\|\\phi(y)\\|^2$ in the feature space.\n",
    "\n",
    "**Norm Squared of $\\phi(x)$ and $\\phi(y)$**:\n",
    "   Let's compute $\\|\\phi(x)\\|^2$:\n",
    "\n",
    "   $$\n",
    "   \\|\\phi(x)\\|^2 = \\phi(x)^T \\phi(x)\n",
    "   $$\n",
    "\n",
    "   Similarly,\n",
    "   \n",
    "   $$\n",
    "   \\|\\phi(y)\\|^2 = \\phi(y)^T \\phi(y)\n",
    "   $$\n",
    "\n",
    "   For the Gaussian kernel:\n",
    "\n",
    "   $$\n",
    "   k(x, x) = \\exp\\left(-\\frac{\\|x - x\\|^2}{2\\sigma^2}\\right) = \\exp(0) = 1\n",
    "   $$\n",
    "\n",
    "   Therefore,\n",
    "\n",
    "   $$\n",
    "   \\|\\phi(x)\\|^2 = \\phi(x)^T \\phi(x) = k(x, x) = 1\n",
    "   $$\n",
    "\n",
    "   Similarly,\n",
    "\n",
    "   $$\n",
    "   \\|\\phi(y)\\|^2 = \\phi(y)^T \\phi(y) = k(y, y) = 1\n",
    "   $$\n",
    "\n",
    "**Compute $\\|\\phi(x) - \\phi(y)\\|^2$**:\n",
    "   Now, using the computed norms and the kernel function:\n",
    "\n",
    "   $$\n",
    "   \\|\\phi(x) - \\phi(y)\\|^2 = \\|\\phi(x)\\|^2 + \\|\\phi(y)\\|^2 - 2 \\phi(x)^T \\phi(y)\n",
    "   $$\n",
    "\n",
    "   Substituting the values:\n",
    "\n",
    "   $$\n",
    "   \\|\\phi(x) - \\phi(y)\\|^2 = 1 + 1 - 2 \\phi(x)^T \\phi(y)\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\|\\phi(x) - \\phi(y)\\|^2 = 2 - 2 \\phi(x)^T \\phi(y)\n",
    "   $$\n",
    "\n",
    "   Since $\\phi(x)^T \\phi(y) = k(x, y)$:\n",
    "\n",
    "   $$\n",
    "   \\|\\phi(x) - \\phi(y)\\|^2 = 2 - 2 k(x, y)\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy C-Means clustering in $ \\phi $ space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kernel Fuzzy C-Means with Kernelization of the Metric**\n",
    "The Kernel Fuzzy C-Means (KFCM) algorithm is an extension of the standard Fuzzy C-Means (FCM) algorithm that uses kernel methods to handle non-linearity by implicitly mapping data into a high-dimensional feature space. \n",
    "\n",
    "***Objective Function***\n",
    "\n",
    "$$\n",
    "J_{KFCM} = \\sum_{i=1}^{c} \\sum_{k=1}^{n} (u_{ik})^m \\|\\psi(x_k) - \\psi(g_i)\\|^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ u_{ik} $ is the membership degree of object $ x_k $ in cluster $ i $.\n",
    "- $ \\psi(x_k) $ and $ \\psi(g_i) $ are the feature space representations of data point $ x_k $ and cluster center $ g_i $, respectively.\n",
    "- $ m $ is the fuzziness parameter.\n",
    "- $ \\|\\psi(x_k) - \\psi(g_i)\\|^2 $ represents the squared Euclidean distance between the feature space representations of $ x_k $ and $ g_i $.\n",
    "\n",
    "Using the kernel trick, this distance can be expressed in terms of the kernel function:\n",
    "\n",
    "$$\n",
    "\\|\\psi(x_k) - \\psi(g_i)\\|^2 = K(x_k, x_k) + K(g_i, g_i) - 2K(x_k, g_i)\n",
    "$$\n",
    "\n",
    "where $ K(\\cdot, \\cdot) $ is the kernel function. For the Gaussian kernel, this becomes:\n",
    "\n",
    "$$\n",
    "K(x_k, x_k) = 1 \\text{ and } K(g_i, g_i) = 1\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\|\\psi(x_k) - \\psi(g_i)\\|^2 = 2 - 2K(x_k, g_i)\n",
    "$$\n",
    "\n",
    "Substitute this into the objective function:\n",
    "\n",
    "$$\n",
    "J_{KFCM} = \\sum_{i=1}^{c} \\sum_{k=1}^{n} (u_{ik})^m (2 - 2K(x_k, g_i))\n",
    "$$\n",
    "\n",
    "which simplifies to:\n",
    "\n",
    "$$\n",
    "J_{KFCM} = 2 \\sum_{i=1}^{c} \\sum_{k=1}^{n} (u_{ik})^m - 2 \\sum_{i=1}^{c} \\sum_{k=1}^{n} (u_{ik})^m K(x_k, g_i)\n",
    "$$\n",
    "\n",
    "***Gaussian Kernel***\n",
    "\n",
    "The Gaussian kernel is defined as:\n",
    "\n",
    "$$\n",
    "K(x_l, x_k) = \\exp \\left( -\\frac{\\|x_l - x_k\\|^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "where $ \\sigma^2 $ is the width parameter of the Gaussian kernel. For the Gaussian kernel:\n",
    "\n",
    "$$\n",
    "\\| \\psi(x_k) - \\psi(g_i) \\|^2 = 2 - 2K(x_k, g_i)\n",
    "$$\n",
    "\n",
    "***Update Rules for Prototypes and Membership Degrees***\n",
    "\n",
    "**Update Prototypes:**\n",
    "\n",
    "To minimize $ J_{KFCM} $ with respect to the cluster prototypes $ g_i $, we need to find the partial derivatives and set them to zero. The update rule for the prototypes $ g_i $ is derived as:\n",
    "\n",
    "$$\n",
    "g_i = \\frac{\\sum_{k=1}^{n} (u_{ik})^m K(x_k, g_i) x_k}{\\sum_{k=1}^{n} (u_{ik})^m K(x_k, g_i)}\n",
    "$$\n",
    "\n",
    "**Update Membership Degrees:**\n",
    "\n",
    "To update the membership degrees $ u_{ik} $, we use the method of Lagrange multipliers with the constraints $ \\sum_{i=1}^{c} u_{ik} = 1 $ and $ u_{ik} \\geq 0 $. The update rule for the membership degrees $ u_{ik} $ is given by:\n",
    "\n",
    "$$\n",
    "u_{ik} = \\frac{1}{\\sum_{h=1}^{c} \\left( \\frac{1 - K(x_k, g_i)}{1 - K(x_k, g_h)} \\right)^{\\frac{2}{m-1}}}\n",
    "$$\n",
    "\n",
    "where $ K(x_k, g_i) $ and $ K(x_k, g_h) $ are kernel evaluations between $ x_k $ and the cluster centers.\n",
    "\n",
    "<span style=\"color:blue\">**Algorithm Steps**</span> \n",
    "\n",
    ". **Initialize** the membership matrix $ U $ and the prototypes $ g_i $ randomly.\n",
    "\n",
    ". **Repeat** until convergence:\n",
    "   - **Update the prototypes** $ g_i $ using:\n",
    "\n",
    "     $$\n",
    "     g_i = \\frac{\\sum_{k=1}^{n} (u_{ik})^m K(x_k, g_i) x_k}{\\sum_{k=1}^{n} (u_{ik})^m K(x_k, g_i)}\n",
    "     $$\n",
    "\n",
    "   - **Update the membership degrees** $ u_{ik} $ using:\n",
    "\n",
    "     $$\n",
    "     u_{ik} = \\frac{1}{\\sum_{h=1}^{c} \\left( \\frac{1 - K(x_k, g_i)}{1 - K(x_k, g_h)} \\right)^{\\frac{2}{m-1}}}\n",
    "     $$\n",
    "\n",
    "**Check for convergence** \n",
    "\n",
    "  $$\n",
    "     \\text{If } \\left| J_{KFCM}^{(k)} - J_{KFCM}^{(k-1)} \\right| \\leq \\text{threshold}, \\text{ then stop}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression in $ \\phi $ space "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Objective Function in Feature Space***\n",
    "\n",
    "We start with the objective function of Ordinary Least Squares (OLS) regression in the feature space $\\phi(x)$:\n",
    "\n",
    "$$ J(w) = \\sum_{i=1}^{n} (y_i - f(x_i))^2 = \\sum_{i=1}^{n} (y_i - w^T \\phi(x_i))^2 $$\n",
    "\n",
    "Expanding the squared term:\n",
    "\n",
    "$$ J(w) = \\sum_{i=1}^{n} (y_i - w^T \\phi(x_i))(y_i - w^T \\phi(x_i)) $$\n",
    "\n",
    "$$ J(w) = \\sum_{i=1}^{n} \\left[ y_i^2 - 2y_i (w^T \\phi(x_i)) + (w^T \\phi(x_i))^2 \\right] $$\n",
    "\n",
    "**Taking the Gradient**\n",
    "\n",
    "To find the optimal $w$, we take the gradient of $J(w)$ with respect to $w$:\n",
    "\n",
    "$$ \\frac{\\partial J(w)}{\\partial w} = \\sum_{i=1}^{n} \\left[ -2 y_i \\phi(x_i) + 2 (w^T \\phi(x_i)) \\phi(x_i) \\right] $$\n",
    "\n",
    "$$ \\frac{\\partial J(w)}{\\partial w} = -2 \\sum_{i=1}^{n} y_i \\phi(x_i) + 2 \\sum_{i=1}^{n} \\phi(x_i) (\\phi(x_i)^T w) $$\n",
    "\n",
    "**Setting the Gradient to Zero**\n",
    "\n",
    "To find the optimal $w$, set the gradient to zero:\n",
    "\n",
    "$$\n",
    "-2 \\sum_{i=1}^{n} y_i \\phi(x_i) + 2 \\sum_{i=1}^{n} \\phi(x_i) (\\phi(x_i)^T w) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} y_i \\phi(x_i) = \\sum_{i=1}^{n} \\phi(x_i) (\\phi(x_i)^T w)\n",
    "$$\n",
    "\n",
    "**Representing $w$ as a Linear Combination of $\\phi(x_i)$**\n",
    "\n",
    "We assume $w$ can be expressed as a linear combination of the feature vectors $\\phi(x_i)$:\n",
    "\n",
    "$$\n",
    "w = \\sum_{i=1}^{n} \\alpha_i \\phi(x_i)\n",
    "$$\n",
    "\n",
    "Substitute this into the equation:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} y_i \\phi(x_i) = \\sum_{i=1}^{n} \\phi(x_i) \\left( \\phi(x_i)^T \\sum_{j=1}^{n} \\alpha_j \\phi(x_j) \\right)\n",
    "$$\n",
    "\n",
    "**Using the Kernel Trick**\n",
    "\n",
    "Notice that $\\phi(x_i)^T \\phi(x_j)$ is the kernel function $k(x_i, x_j)$. Therefore,\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} y_i \\phi(x_i) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_j \\phi(x_i) k(x_i, x_j)\n",
    "$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} y_i \\phi(x_i) = \\sum_{i=1}^{n} \\phi(x_i) \\left( \\sum_{j=1}^{n} \\alpha_j k(x_i, x_j) \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "***Solving for $\\alpha$***\n",
    "\n",
    "$ y $ is the column vector of labels $ [y_1, y_2, \\ldots, y_n]^T $.\n",
    "\n",
    "Similarly, let $ \\alpha $ be the column vector of coefficients $ [\\alpha_1, \\alpha_2, \\ldots, \\alpha_n]^T $\n",
    "After inner product into $\\phi(x_k)$, We reached the equation:\n",
    "\n",
    "$$ \\sum_{i=1}^{n} y_i k(x_k, x_i) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_j k(x_k, x_i) k(x_i, x_j) $$\n",
    "\n",
    "We simplify this to:\n",
    "\n",
    "$$ \\sum_{i=1}^{n} y_i k(x_k, x_i) = \\sum_{j=1}^{n} \\alpha_j \\sum_{i=1}^{n} k(x_k, x_i) k(x_i, x_j) $$\n",
    "\n",
    "For all $ x_k $ where $ k = 1, \\ldots, n $, we have $ n $ system equations. After solving them, we obtain $ \\alpha $.\"\n",
    "\n",
    "**Making Predictions**\n",
    "\n",
    "Using the obtained $\\alpha$, the regression function $f(x)$ for a new input $x$ is given by:\n",
    "\n",
    "$$\n",
    "f(x) = w^T \\phi(x) = \\left( \\sum_{i=1}^{n} \\alpha_i \\phi(x_i) \\right)^T \\phi(x)\n",
    "$$\n",
    "\n",
    "By the kernel trick, we have:\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^{n} \\alpha_i k(x_i, x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The weight vector norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Formulation***\n",
    "\n",
    "You appear to be discussing the representation of a weight vector $ w $ in terms of kernel functions and dual coefficients. Here’s how these expressions are generally interpreted in kernel methods:\n",
    "\n",
    "**Weight Vector Representation**\n",
    "\n",
    "In kernel methods, the weight vector $ w $ in the feature space can be expressed as a linear combination of the transformed data points. If $ \\phi(x_i) $ denotes the feature space transformation of $ x_i $, then:\n",
    "\n",
    "$$\n",
    "w = \\sum_{i=1}^{n} \\alpha_i \\phi(x_i)\n",
    "$$\n",
    "\n",
    "where $ \\alpha_i $ are the coefficients in the dual space, and $ n $ is the number of training samples.\n",
    "\n",
    "**Inner Product in Feature Space**\n",
    "\n",
    "The inner product between two weight vectors $ w $ and $ w $ can be expressed as:\n",
    "\n",
    "$$\n",
    "w^T w = \\left( \\sum_{i=1}^{n} \\alpha_i \\phi(x_i) \\right)^T \\left( \\sum_{j=1}^{n} \\alpha_j \\phi(x_j) \\right)\n",
    "$$\n",
    "\n",
    "Expanding this expression:\n",
    "\n",
    "$$\n",
    "w^T w = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j \\phi(x_i)^T \\phi(x_j)\n",
    "$$\n",
    "\n",
    "In kernel methods, the inner product $ \\phi(x_i)^T \\phi(x_j) $ in the feature space is replaced by the kernel function $ K(x_i, x_j) $:\n",
    "\n",
    "$$\n",
    "\\phi(x_i)^T \\phi(x_j) = K(x_i, x_j)\n",
    "$$\n",
    "\n",
    "Thus, the weight vector's inner product can be written as:\n",
    "\n",
    "$$\n",
    "w^T w = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j K(x_i, x_j)\n",
    "$$\n",
    "\n",
    "**Kernel Matrix Representation**\n",
    "\n",
    "The kernel matrix $ K $ is defined as:\n",
    "\n",
    "$$\n",
    "K_{ij} = K(x_i, x_j)\n",
    "$$\n",
    "\n",
    "Therefore, the inner product of weight vectors $ w $ and $ w $ can also be expressed as:\n",
    "\n",
    "$$\n",
    "w^T w = \\alpha^T K \\alpha\n",
    "$$\n",
    "\n",
    "where $ \\alpha $ is the vector of coefficients $ \\alpha_i $, and $ K $ is the kernel matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization transformation you mentioned requires scaling the kernel function to ensure it is normalized with respect to the data points. This adjustment can be particularly useful in various machine learning algorithms that utilize kernel methods. Let's analyze the normalization process and derive the normalized kernel function step by step.\n",
    "\n",
    "***Normalization Transformation***\n",
    "\n",
    "Given the transformation:\n",
    "\n",
    "$$\n",
    "\\hat{\\phi}(x) = \\frac{\\phi(x)}{\\|\\phi(x)\\|}\n",
    "$$\n",
    "\n",
    "where $ \\phi(x) $ is the feature space representation of $ x $, and $ \\|\\phi(x)\\| $ denotes the norm of $ \\phi(x) $ in the feature space. \n",
    "\n",
    "For two data points $ x $ and $ z $, the normalized kernel function $ \\hat{\\kappa}(x, z) $ is defined as:\n",
    "\n",
    "$$\n",
    "\\hat{\\kappa}(x, z) = \\langle \\hat{\\phi}(x), \\hat{\\phi}(z) \\rangle\n",
    "$$\n",
    "\n",
    "Substitute $ \\hat{\\phi}(x) $ and $ \\hat{\\phi}(z) $ into the inner product:\n",
    "\n",
    "$$\n",
    "\\hat{\\kappa}(x, z) = \\left\\langle \\frac{\\phi(x)}{\\|\\phi(x)\\|}, \\frac{\\phi(z)}{\\|\\phi(z)\\|} \\right\\rangle\n",
    "$$\n",
    "\n",
    "**Calculating the Normalized Kernel Function**\n",
    "\n",
    "**Compute the Inner Product**:\n",
    "\n",
    "$$\n",
    "\\hat{\\kappa}(x, z) = \\frac{\\langle \\phi(x), \\phi(z) \\rangle}{\\|\\phi(x)\\| \\|\\phi(z)\\|}\n",
    "$$\n",
    "\n",
    "Here, $ \\langle \\phi(x), \\phi(z) \\rangle $ is the dot product in the feature space, which is equal to the kernel function $ \\kappa(x, z) $:\n",
    "\n",
    "$$\n",
    "\\langle \\phi(x), \\phi(z) \\rangle = \\kappa(x, z)\n",
    "$$\n",
    "\n",
    "**Compute Norms**:\n",
    "\n",
    "The norm of $ \\phi(x) $ in the feature space is:\n",
    "\n",
    "$$\n",
    "\\|\\phi(x)\\| = \\sqrt{\\langle \\phi(x), \\phi(x) \\rangle} = \\sqrt{\\kappa(x, x)}\n",
    "$$\n",
    "\n",
    "Similarly:\n",
    "\n",
    "$$\n",
    "\\|\\phi(z)\\| = \\sqrt{\\langle \\phi(z), \\phi(z) \\rangle} = \\sqrt{\\kappa(z, z)}\n",
    "$$\n",
    "\n",
    "**Substitute Norms**:\n",
    "\n",
    "Substitute these norms into the normalized kernel function:\n",
    "\n",
    "$$\n",
    "\\hat{\\kappa}(x, z) = \\frac{\\kappa(x, z)}{\\sqrt{\\kappa(x, x)} \\sqrt{\\kappa(z, z)}}\n",
    "$$\n",
    "\n",
    "This can also be written as:\n",
    "\n",
    "$$\n",
    "\\hat{\\kappa}(x, z) = \\frac{\\kappa(x, z)}{\\sqrt{\\kappa(x, x) \\kappa(z, z)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Norm and distance from the centre of mass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept you’re discussing involves calculating and interpreting the center of mass (or centroid) in the feature space induced by a kernel function, and its properties. Here's a structured explanation and derivation of the key points:\n",
    "\n",
    "### Center of Mass in Feature Space\n",
    "\n",
    "Given a set of data points $ \\{ x_i \\}_{i=1}^n $ mapped to the feature space $ \\phi(x_i) $, the center of mass (or centroid) of the set $ \\{\\phi(x_i)\\} $ is:\n",
    "\n",
    "$$\n",
    "\\phi_S = \\frac{1}{n} \\sum_{i=1}^n \\phi(x_i)\n",
    "$$\n",
    "\n",
    "***Norm of the Center of Mass***\n",
    "\n",
    "To compute the norm squared of $ \\phi_S $, we use:\n",
    "\n",
    "$$\n",
    "\\|\\phi_S\\|^2 = \\langle \\phi_S, \\phi_S \\rangle\n",
    "$$\n",
    "\n",
    "Substitute $ \\phi_S $ into this equation:\n",
    "\n",
    "$$\n",
    "\\|\\phi_S\\|^2 = \\left\\langle \\frac{1}{n} \\sum_{i=1}^n \\phi(x_i), \\frac{1}{n} \\sum_{j=1}^n \\phi(x_j) \\right\\rangle\n",
    "$$\n",
    "\n",
    "Expand the inner product:\n",
    "\n",
    "$$\n",
    "\\|\\phi_S\\|^2 = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\langle \\phi(x_i), \\phi(x_j) \\rangle\n",
    "$$\n",
    "\n",
    "Using the kernel function $ \\kappa(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle $, we get:\n",
    "\n",
    "$$\n",
    "\\|\\phi_S\\|^2 = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\kappa(x_i, x_j)\n",
    "$$\n",
    "\n",
    "The term $ \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\kappa(x_i, x_j) $ is the average of all entries in the kernel matrix $ K $. Hence:\n",
    "\n",
    "$$\n",
    "\\|\\phi_S\\|^2 = \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\kappa(x_i, x_j)\n",
    "$$\n",
    "\n",
    "**Distance from a Point to the Center of Mass**\n",
    "\n",
    "For a data point $ x $, the squared distance from $ \\phi(x) $ to $ \\phi_S $ is:\n",
    "\n",
    "$$\n",
    "\\|\\phi(x) - \\phi_S\\|^2\n",
    "$$\n",
    "\n",
    "Using the identity:\n",
    "\n",
    "$$\n",
    "\\|\\phi(x) - \\phi_S\\|^2 = \\|\\phi(x)\\|^2 + \\|\\phi_S\\|^2 - 2 \\langle \\phi(x), \\phi_S \\rangle\n",
    "$$\n",
    "\n",
    "Substitute $ \\|\\phi(x)\\|^2 = \\kappa(x, x) $ and $ \\langle \\phi(x), \\phi_S \\rangle = \\frac{1}{n} \\sum_{i=1}^n \\kappa(x, x_i) $:\n",
    "\n",
    "$$\n",
    "\\|\\phi(x) - \\phi_S\\|^2 = \\kappa(x, x) + \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\kappa(x_i, x_j) - \\frac{2}{n} \\sum_{i=1}^n \\kappa(x, x_i)\n",
    "$$\n",
    "\n",
    "### Expected Squared Distance from Center of Mass\n",
    "\n",
    "For a set of points $ \\{x_s\\}_{s=1}^s $:\n",
    "\n",
    "$$\n",
    "\\frac{1}{s} \\sum_{s=1}^s \\|\\phi(x_s) - \\phi_S\\|^2\n",
    "$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "\\frac{1}{s} \\sum_{s=1}^s \\|\\phi(x_s) - \\phi_S\\|^2 = \\frac{1}{s} \\sum_{s=1}^s \\left( \\kappa(x_s, x_s) + \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\kappa(x_i, x_j) - \\frac{2}{n} \\sum_{i=1}^n \\kappa(x_s, x_i) \\right)\n",
    "$$\n",
    "\n",
    "Rearrange:\n",
    "\n",
    "$$\n",
    "\\frac{1}{s} \\sum_{s=1}^s \\|\\phi(x_s) - \\phi_S\\|^2 = \\frac{1}{s} \\sum_{s=1}^s \\kappa(x_s, x_s) - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\kappa(x_i, x_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the initial objective function for regression with ε-insensitive loss:\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{d} w_j^2\n",
    "$$\n",
    "\n",
    "and substituting the square loss with the ε-insensitive loss, the objective function becomes:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} \\max(0, |e_i| - \\epsilon)\n",
    "$$\n",
    "\n",
    "where $ e_i = y_i - \\hat{y}_i = y_i - (w^T x_i + b) $.\n",
    "\n",
    "**Introducing Slack Variables**\n",
    "\n",
    "We introduce slack variables $ \\xi_i $ to represent the amount by which each error $ |e_i| $ exceeds $ \\epsilon $:\n",
    "\n",
    "$$\n",
    "\\xi_i = \\max(0, |e_i| - \\epsilon)\n",
    "$$\n",
    "\n",
    "Thus, the objective function can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} \\xi_i\n",
    "$$\n",
    "\n",
    "**Constraints**\n",
    "\n",
    "The slack variables $ \\xi_i $ must satisfy the following constraints:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "y_i - (w^T x_i + b) \\leq \\epsilon + \\xi_i \\\\\n",
    "(w^T x_i + b) - y_i \\leq \\epsilon + \\xi_i \\\\\n",
    "\\xi_i \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "To handle this constraint without using the absolute value, we can split it into two separate constraints:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "y_i - (w^T x_i + b) \\leq \\epsilon + \\xi_i \\\\\n",
    "(w^T x_i + b) - y_i \\leq \\epsilon + \\xi_i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Additionally, we need to ensure that $ \\xi_i \\geq 0 $.\n",
    "\n",
    "***Primal Problem***\n",
    "\n",
    "Combining these constraints with the objective function, we get the primal problem:\n",
    "\n",
    "$$\n",
    "\\min_{w, b, \\xi} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} \\xi_i\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "y_i - (w^T x_i + b) \\leq \\epsilon + \\xi_i \\\\\n",
    "(w^T x_i + b) - y_i \\leq \\epsilon + \\xi_i \\\\\n",
    "\\xi_i \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "_Using Two Slack Variables_\n",
    "\n",
    "To handle positive and negative deviations separately, we introduce two slack variables $ \\xi_i $ and $ \\xi_i^* $. The formulation is as follows:\n",
    "\n",
    "**_Objective Function_**\n",
    "\n",
    "$$\n",
    "\\min_{w, b, \\xi, \\xi^*} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} (\\xi_i + \\xi_i^*)\n",
    "$$\n",
    "\n",
    "**Constraints**\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "y_i - (w^T x_i + b) \\leq \\epsilon + \\xi_i \\\\\n",
    "(w^T x_i + b) - y_i \\leq \\epsilon + \\xi_i^* \\\\\n",
    "\\xi_i, \\xi_i^* \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Final Formulation**\n",
    "\n",
    "Thus, the final formulation of the Support Vector Regression (SVR) with ε-insensitive loss using two slack variables is:\n",
    "\n",
    "$$\n",
    "\\min_{w, b, \\xi, \\xi^*} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} (\\xi_i + \\xi_i^*)\n",
    "$$\n",
    "\n",
    "subject to the constraints:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "y_i - (w^T x_i + b) \\leq \\epsilon + \\xi_i \\\\\n",
    "(w^T x_i + b) - y_i \\leq \\epsilon + \\xi_i^* \\\\\n",
    "\\xi_i, \\xi_i^* \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### SVR in Feature Space with Kernel Trick\n",
    "\n",
    "To implement SVR in the feature space using the kernel trick, we replace the inner product $ w^T x $ with a kernel function $ \\kappa(x, x') $:\n",
    "\n",
    "#### Primal Formulation\n",
    "\n",
    "The primal formulation in the feature space $ \\phi $ is:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}, b, \\xi, \\xi^*} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{n} (\\xi_i + \\xi_i^*)\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "y_i - (\\mathbf{w}^T \\phi(x_i) + b) \\leq \\epsilon + \\xi_i \\\\\n",
    "(\\mathbf{w}^T \\phi(x_i) + b) - y_i \\leq \\epsilon + \\xi_i^* \\\\\n",
    "\\xi_i, \\xi_i^* \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### Dual Formulation\n",
    "\n",
    "The dual formulation of the above problem using Lagrange multipliers $ \\alpha_i $ and $ \\alpha_i^* $ is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\max_{\\alpha, \\alpha^*} \\sum_{i=1}^{n} ( \\alpha_i - \\alpha_i^* ) y_i - \\epsilon \\sum_{i=1}^{n} ( \\alpha_i + \\alpha_i^* ) \\\\\n",
    "&- \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} ( \\alpha_i - \\alpha_i^* )( \\alpha_j - \\alpha_j^* ) \\kappa(x_i, x_j)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\sum_{i=1}^{n} ( \\alpha_i - \\alpha_i^* ) = 0 \\\\\n",
    "0 \\leq \\alpha_i, \\alpha_i^* \\leq C\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Here, $ \\kappa(x_i, x_j) $ is the kernel function that computes the dot product in the high-dimensional feature space.\n",
    "\n",
    "#### Final Prediction\n",
    "\n",
    "The final prediction for a test point $ x $ is given by:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sum_{i=1}^{n} ( \\alpha_i - \\alpha_i^* ) \\kappa(x_i, x) + b\n",
    "$$\n",
    "\n",
    "where $ \\alpha_i $ and $ \\alpha_i^* $ are the solutions to the dual problem and $ b $ is the bias term.\n",
    "\n",
    "This SVR formulation in the feature space allows us to leverage the kernel trick to handle nonlinear regression tasks efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step Derivation of Support Vector Regression (SVR) in Kernel Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective and Constraints in the Primal Form\n",
    "\n",
    "The primal problem for SVR with $\\epsilon$-insensitive loss can be formulated as:\n",
    "\n",
    "Objective function:\n",
    "\n",
    "$$\n",
    "\\min_{w, b, \\xi, \\xi^*} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} (\\xi_i + \\xi_i^*)\n",
    "$$\n",
    "\n",
    "Subject to constraints:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "y_i - (w^T \\phi(x_i) + b) \\leq \\epsilon + \\xi_i \\\\\n",
    "(w^T \\phi(x_i) + b) - y_i \\leq \\epsilon + \\xi_i^* \\\\\n",
    "\\xi_i, \\xi_i^* \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $w$ is the weight vector in the feature space, $b$ is the bias term, $\\xi_i$ and $\\xi_i^*$ are slack variables for positive and negative deviations, respectively, and $C$ is a regularization parameter.\n",
    "\n",
    "#### Lagrangian and Dual Problem\n",
    "\n",
    "To solve the primal problem, we first construct the Lagrangian function. Introduce Lagrange multipliers $\\alpha_i, \\alpha_i^*, \\eta_i, \\eta_i^* \\geq 0$ for the constraints:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(w, b, \\xi, \\xi^*, \\alpha, \\alpha^*, \\eta, \\eta^*) &= \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} (\\xi_i + \\xi_i^*) \\\\\n",
    "&\\quad - \\sum_{i=1}^{n} \\alpha_i [\\epsilon + \\xi_i - y_i + (w^T \\phi(x_i) + b)] \\\\\n",
    "&\\quad - \\sum_{i=1}^{n} \\alpha_i^* [\\epsilon + \\xi_i^* + y_i - (w^T \\phi(x_i) + b)] \\\\\n",
    "&\\quad - \\sum_{i=1}^{n} \\eta_i \\xi_i - \\sum_{i=1}^{n} \\eta_i^* \\xi_i^*\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### Stationarity Conditions\n",
    "(stationary points are those points where the partial derivatives of Λ are zero)\n",
    "\n",
    "To find the dual problem, we need to set the partial derivatives of the Lagrangian with respect to the primal variables to zero.\n",
    "\n",
    "**With respect to $w$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = w - \\sum_{i=1}^{n} (\\alpha_i - \\alpha_i^*) \\phi(x_i) = 0 \\implies w = \\sum_{i=1}^{n} (\\alpha_i - \\alpha_i^*) \\phi(x_i)\n",
    "$$\n",
    "\n",
    "**With respect to $b$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = - \\sum_{i=1}^{n} (\\alpha_i - \\alpha_i^*) = 0 \\implies \\sum_{i=1}^{n} (\\alpha_i - \\alpha_i^*) = 0\n",
    "$$\n",
    "\n",
    "**With respect to $\\xi_i$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\xi_i} = C - \\alpha_i - \\eta_i = 0 \\implies \\alpha_i = C - \\eta_i\n",
    "$$\n",
    "\n",
    "**With respect to $\\xi_i^*$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\xi_i^*} = C - \\alpha_i^* - \\eta_i^* = 0 \\implies \\alpha_i^* = C - \\eta_i^*\n",
    "$$\n",
    "\n",
    "#### Substituting Back\n",
    "\n",
    "Substituting the stationarity conditions back into the Lagrangian, we get the dual problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L_D &= \\frac{1}{2} \\|w\\|^2 - \\sum_{i=1}^{n} \\alpha_i [\\epsilon + \\xi_i - y_i + (w^T \\phi(x_i) + b)] \\\\\n",
    "&\\quad - \\sum_{i=1}^{n} \\alpha_i^* [\\epsilon + \\xi_i^* + y_i - (w^T \\phi(x_i) + b)]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since $ w = \\sum_{i=1}^{n} (\\alpha_i - \\alpha_i^*) \\phi(x_i) $, substitute $ w $ into the Lagrangian:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\|\\sum_{i=1}^{n} (\\alpha_i - \\alpha_i^*) \\phi(x_i)\\|^2\n",
    "$$\n",
    "\n",
    "This expands to:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} (\\alpha_i - \\alpha_i^*) (\\alpha_j - \\alpha_j^*) \\langle \\phi(x_i), \\phi(x_j) \\rangle\n",
    "$$\n",
    "\n",
    "Using the kernel trick $ \\kappa(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle $:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} (\\alpha_i - \\alpha_i^*) (\\alpha_j - \\alpha_j^*) \\kappa(x_i, x_j)\n",
    "$$\n",
    "\n",
    "#### Dual Problem\n",
    "\n",
    "Combining all the terms, the dual problem becomes:\n",
    "\n",
    "Maximize:\n",
    "\n",
    "$$\n",
    "W(\\alpha, \\alpha^*) = -\\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} (\\alpha_i - \\alpha_i^*) (\\alpha_j - \\alpha_j^*) \\kappa(x_i, x_j) - \\epsilon \\sum_{i=1}^{n} (\\alpha_i + \\alpha_i^*) + \\sum_{i=1}^{n} y_i (\\alpha_i - \\alpha_i^*)\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\sum_{i=1}^{n} (\\alpha_i - \\alpha_i^*) = 0 \\\\\n",
    "0 \\leq \\alpha_i, \\alpha_i^* \\leq C\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### Prediction\n",
    "\n",
    "The prediction function for a new data point $ x $ is given by:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sum_{i=1}^{n} (\\alpha_i - \\alpha_i^*) \\kappa(x_i, x) + b\n",
    "$$\n",
    "\n",
    "where $ b $ can be determined by exploiting the Karush-Kuhn-Tucker (KKT) conditions.\n",
    "\n",
    "#### Determining $ b $\n",
    "\n",
    "To determine $ b $, we use the support vectors (data points with $ 0 < \\alpha_i < C $ or $ 0 < \\alpha_i^* < C $):\n",
    "\n",
    "$$\n",
    "b = y_i - \\sum_{j=1}^{n} (\\alpha_j - \\alpha_j^*) \\kappa(x_j, x_i) - \\epsilon \\quad \\text{if} \\quad 0 < \\alpha_i < C\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "b = y_i - \\sum_{j=1}^{n} (\\alpha_j - \\alpha_j^*) \\kappa(x_j, x_i) + \\epsilon \\quad \\text{if} \\quad 0 < \\alpha_i^* < C\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
